// Copyright 2017 Google Inc.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

// @generated by protoc-gen-es v1.3.0 with parameter "target=ts"
// @generated from file google/cloud/videointelligence/v1beta2/video_intelligence.proto (package google.cloud.videointelligence.v1beta2, syntax proto3)
/* eslint-disable */
// @ts-nocheck

import type { BinaryReadOptions, FieldList, JsonReadOptions, JsonValue, PartialMessage, PlainMessage } from "@bufbuild/protobuf";
import { Duration, Message, proto3, Timestamp } from "@bufbuild/protobuf";
import { Status } from "../../../rpc/status_pb.js";

/**
 * Video annotation feature.
 *
 * @generated from enum google.cloud.videointelligence.v1beta2.Feature
 */
export enum Feature {
  /**
   * Unspecified.
   *
   * @generated from enum value: FEATURE_UNSPECIFIED = 0;
   */
  FEATURE_UNSPECIFIED = 0,

  /**
   * Label detection. Detect objects, such as dog or flower.
   *
   * @generated from enum value: LABEL_DETECTION = 1;
   */
  LABEL_DETECTION = 1,

  /**
   * Shot change detection.
   *
   * @generated from enum value: SHOT_CHANGE_DETECTION = 2;
   */
  SHOT_CHANGE_DETECTION = 2,

  /**
   * Explicit content detection.
   *
   * @generated from enum value: EXPLICIT_CONTENT_DETECTION = 3;
   */
  EXPLICIT_CONTENT_DETECTION = 3,

  /**
   * Human face detection and tracking.
   *
   * @generated from enum value: FACE_DETECTION = 4;
   */
  FACE_DETECTION = 4,
}
// Retrieve enum metadata with: proto3.getEnumType(Feature)
proto3.util.setEnumType(Feature, "google.cloud.videointelligence.v1beta2.Feature", [
  { no: 0, name: "FEATURE_UNSPECIFIED" },
  { no: 1, name: "LABEL_DETECTION" },
  { no: 2, name: "SHOT_CHANGE_DETECTION" },
  { no: 3, name: "EXPLICIT_CONTENT_DETECTION" },
  { no: 4, name: "FACE_DETECTION" },
]);

/**
 * Label detection mode.
 *
 * @generated from enum google.cloud.videointelligence.v1beta2.LabelDetectionMode
 */
export enum LabelDetectionMode {
  /**
   * Unspecified.
   *
   * @generated from enum value: LABEL_DETECTION_MODE_UNSPECIFIED = 0;
   */
  LABEL_DETECTION_MODE_UNSPECIFIED = 0,

  /**
   * Detect shot-level labels.
   *
   * @generated from enum value: SHOT_MODE = 1;
   */
  SHOT_MODE = 1,

  /**
   * Detect frame-level labels.
   *
   * @generated from enum value: FRAME_MODE = 2;
   */
  FRAME_MODE = 2,

  /**
   * Detect both shot-level and frame-level labels.
   *
   * @generated from enum value: SHOT_AND_FRAME_MODE = 3;
   */
  SHOT_AND_FRAME_MODE = 3,
}
// Retrieve enum metadata with: proto3.getEnumType(LabelDetectionMode)
proto3.util.setEnumType(LabelDetectionMode, "google.cloud.videointelligence.v1beta2.LabelDetectionMode", [
  { no: 0, name: "LABEL_DETECTION_MODE_UNSPECIFIED" },
  { no: 1, name: "SHOT_MODE" },
  { no: 2, name: "FRAME_MODE" },
  { no: 3, name: "SHOT_AND_FRAME_MODE" },
]);

/**
 * Bucketized representation of likelihood.
 *
 * @generated from enum google.cloud.videointelligence.v1beta2.Likelihood
 */
export enum Likelihood {
  /**
   * Unspecified likelihood.
   *
   * @generated from enum value: LIKELIHOOD_UNSPECIFIED = 0;
   */
  LIKELIHOOD_UNSPECIFIED = 0,

  /**
   * Very unlikely.
   *
   * @generated from enum value: VERY_UNLIKELY = 1;
   */
  VERY_UNLIKELY = 1,

  /**
   * Unlikely.
   *
   * @generated from enum value: UNLIKELY = 2;
   */
  UNLIKELY = 2,

  /**
   * Possible.
   *
   * @generated from enum value: POSSIBLE = 3;
   */
  POSSIBLE = 3,

  /**
   * Likely.
   *
   * @generated from enum value: LIKELY = 4;
   */
  LIKELY = 4,

  /**
   * Very likely.
   *
   * @generated from enum value: VERY_LIKELY = 5;
   */
  VERY_LIKELY = 5,
}
// Retrieve enum metadata with: proto3.getEnumType(Likelihood)
proto3.util.setEnumType(Likelihood, "google.cloud.videointelligence.v1beta2.Likelihood", [
  { no: 0, name: "LIKELIHOOD_UNSPECIFIED" },
  { no: 1, name: "VERY_UNLIKELY" },
  { no: 2, name: "UNLIKELY" },
  { no: 3, name: "POSSIBLE" },
  { no: 4, name: "LIKELY" },
  { no: 5, name: "VERY_LIKELY" },
]);

/**
 * Video annotation request.
 *
 * @generated from message google.cloud.videointelligence.v1beta2.AnnotateVideoRequest
 */
export class AnnotateVideoRequest extends Message<AnnotateVideoRequest> {
  /**
   * Input video location. Currently, only
   * [Google Cloud Storage](https://cloud.google.com/storage/) URIs are
   * supported, which must be specified in the following format:
   * `gs://bucket-id/object-id` (other URI formats return
   * [google.rpc.Code.INVALID_ARGUMENT][google.rpc.Code.INVALID_ARGUMENT]). For more information, see
   * [Request URIs](/storage/docs/reference-uris).
   * A video URI may include wildcards in `object-id`, and thus identify
   * multiple videos. Supported wildcards: '*' to match 0 or more characters;
   * '?' to match 1 character. If unset, the input video should be embedded
   * in the request as `input_content`. If set, `input_content` should be unset.
   *
   * @generated from field: string input_uri = 1;
   */
  inputUri = "";

  /**
   * The video data bytes.
   * If unset, the input video(s) should be specified via `input_uri`.
   * If set, `input_uri` should be unset.
   *
   * @generated from field: bytes input_content = 6;
   */
  inputContent = new Uint8Array(0);

  /**
   * Requested video annotation features.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1beta2.Feature features = 2;
   */
  features: Feature[] = [];

  /**
   * Additional video context and/or feature-specific parameters.
   *
   * @generated from field: google.cloud.videointelligence.v1beta2.VideoContext video_context = 3;
   */
  videoContext?: VideoContext;

  /**
   * Optional location where the output (in JSON format) should be stored.
   * Currently, only [Google Cloud Storage](https://cloud.google.com/storage/)
   * URIs are supported, which must be specified in the following format:
   * `gs://bucket-id/object-id` (other URI formats return
   * [google.rpc.Code.INVALID_ARGUMENT][google.rpc.Code.INVALID_ARGUMENT]). For more information, see
   * [Request URIs](/storage/docs/reference-uris).
   *
   * @generated from field: string output_uri = 4;
   */
  outputUri = "";

  /**
   * Optional cloud region where annotation should take place. Supported cloud
   * regions: `us-east1`, `us-west1`, `europe-west1`, `asia-east1`. If no region
   * is specified, a region will be determined based on video file location.
   *
   * @generated from field: string location_id = 5;
   */
  locationId = "";

  constructor(data?: PartialMessage<AnnotateVideoRequest>) {
    super();
    proto3.util.initPartial(data, this);
  }

  static readonly runtime: typeof proto3 = proto3;
  static readonly typeName = "google.cloud.videointelligence.v1beta2.AnnotateVideoRequest";
  static readonly fields: FieldList = proto3.util.newFieldList(() => [
    { no: 1, name: "input_uri", kind: "scalar", T: 9 /* ScalarType.STRING */ },
    { no: 6, name: "input_content", kind: "scalar", T: 12 /* ScalarType.BYTES */ },
    { no: 2, name: "features", kind: "enum", T: proto3.getEnumType(Feature), repeated: true },
    { no: 3, name: "video_context", kind: "message", T: VideoContext },
    { no: 4, name: "output_uri", kind: "scalar", T: 9 /* ScalarType.STRING */ },
    { no: 5, name: "location_id", kind: "scalar", T: 9 /* ScalarType.STRING */ },
  ]);

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): AnnotateVideoRequest {
    return new AnnotateVideoRequest().fromBinary(bytes, options);
  }

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): AnnotateVideoRequest {
    return new AnnotateVideoRequest().fromJson(jsonValue, options);
  }

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): AnnotateVideoRequest {
    return new AnnotateVideoRequest().fromJsonString(jsonString, options);
  }

  static equals(a: AnnotateVideoRequest | PlainMessage<AnnotateVideoRequest> | undefined, b: AnnotateVideoRequest | PlainMessage<AnnotateVideoRequest> | undefined): boolean {
    return proto3.util.equals(AnnotateVideoRequest, a, b);
  }
}

/**
 * Video context and/or feature-specific parameters.
 *
 * @generated from message google.cloud.videointelligence.v1beta2.VideoContext
 */
export class VideoContext extends Message<VideoContext> {
  /**
   * Video segments to annotate. The segments may overlap and are not required
   * to be contiguous or span the whole video. If unspecified, each video
   * is treated as a single segment.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1beta2.VideoSegment segments = 1;
   */
  segments: VideoSegment[] = [];

  /**
   * Config for LABEL_DETECTION.
   *
   * @generated from field: google.cloud.videointelligence.v1beta2.LabelDetectionConfig label_detection_config = 2;
   */
  labelDetectionConfig?: LabelDetectionConfig;

  /**
   * Config for SHOT_CHANGE_DETECTION.
   *
   * @generated from field: google.cloud.videointelligence.v1beta2.ShotChangeDetectionConfig shot_change_detection_config = 3;
   */
  shotChangeDetectionConfig?: ShotChangeDetectionConfig;

  /**
   * Config for EXPLICIT_CONTENT_DETECTION.
   *
   * @generated from field: google.cloud.videointelligence.v1beta2.ExplicitContentDetectionConfig explicit_content_detection_config = 4;
   */
  explicitContentDetectionConfig?: ExplicitContentDetectionConfig;

  /**
   * Config for FACE_DETECTION.
   *
   * @generated from field: google.cloud.videointelligence.v1beta2.FaceDetectionConfig face_detection_config = 5;
   */
  faceDetectionConfig?: FaceDetectionConfig;

  constructor(data?: PartialMessage<VideoContext>) {
    super();
    proto3.util.initPartial(data, this);
  }

  static readonly runtime: typeof proto3 = proto3;
  static readonly typeName = "google.cloud.videointelligence.v1beta2.VideoContext";
  static readonly fields: FieldList = proto3.util.newFieldList(() => [
    { no: 1, name: "segments", kind: "message", T: VideoSegment, repeated: true },
    { no: 2, name: "label_detection_config", kind: "message", T: LabelDetectionConfig },
    { no: 3, name: "shot_change_detection_config", kind: "message", T: ShotChangeDetectionConfig },
    { no: 4, name: "explicit_content_detection_config", kind: "message", T: ExplicitContentDetectionConfig },
    { no: 5, name: "face_detection_config", kind: "message", T: FaceDetectionConfig },
  ]);

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): VideoContext {
    return new VideoContext().fromBinary(bytes, options);
  }

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): VideoContext {
    return new VideoContext().fromJson(jsonValue, options);
  }

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): VideoContext {
    return new VideoContext().fromJsonString(jsonString, options);
  }

  static equals(a: VideoContext | PlainMessage<VideoContext> | undefined, b: VideoContext | PlainMessage<VideoContext> | undefined): boolean {
    return proto3.util.equals(VideoContext, a, b);
  }
}

/**
 * Config for LABEL_DETECTION.
 *
 * @generated from message google.cloud.videointelligence.v1beta2.LabelDetectionConfig
 */
export class LabelDetectionConfig extends Message<LabelDetectionConfig> {
  /**
   * What labels should be detected with LABEL_DETECTION, in addition to
   * video-level labels or segment-level labels.
   * If unspecified, defaults to `SHOT_MODE`.
   *
   * @generated from field: google.cloud.videointelligence.v1beta2.LabelDetectionMode label_detection_mode = 1;
   */
  labelDetectionMode = LabelDetectionMode.LABEL_DETECTION_MODE_UNSPECIFIED;

  /**
   * Whether the video has been shot from a stationary (i.e. non-moving) camera.
   * When set to true, might improve detection accuracy for moving objects.
   * Should be used with `SHOT_AND_FRAME_MODE` enabled.
   *
   * @generated from field: bool stationary_camera = 2;
   */
  stationaryCamera = false;

  /**
   * Model to use for label detection.
   * Supported values: "builtin/stable" (the default if unset) and
   * "builtin/latest".
   *
   * @generated from field: string model = 3;
   */
  model = "";

  constructor(data?: PartialMessage<LabelDetectionConfig>) {
    super();
    proto3.util.initPartial(data, this);
  }

  static readonly runtime: typeof proto3 = proto3;
  static readonly typeName = "google.cloud.videointelligence.v1beta2.LabelDetectionConfig";
  static readonly fields: FieldList = proto3.util.newFieldList(() => [
    { no: 1, name: "label_detection_mode", kind: "enum", T: proto3.getEnumType(LabelDetectionMode) },
    { no: 2, name: "stationary_camera", kind: "scalar", T: 8 /* ScalarType.BOOL */ },
    { no: 3, name: "model", kind: "scalar", T: 9 /* ScalarType.STRING */ },
  ]);

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): LabelDetectionConfig {
    return new LabelDetectionConfig().fromBinary(bytes, options);
  }

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): LabelDetectionConfig {
    return new LabelDetectionConfig().fromJson(jsonValue, options);
  }

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): LabelDetectionConfig {
    return new LabelDetectionConfig().fromJsonString(jsonString, options);
  }

  static equals(a: LabelDetectionConfig | PlainMessage<LabelDetectionConfig> | undefined, b: LabelDetectionConfig | PlainMessage<LabelDetectionConfig> | undefined): boolean {
    return proto3.util.equals(LabelDetectionConfig, a, b);
  }
}

/**
 * Config for SHOT_CHANGE_DETECTION.
 *
 * @generated from message google.cloud.videointelligence.v1beta2.ShotChangeDetectionConfig
 */
export class ShotChangeDetectionConfig extends Message<ShotChangeDetectionConfig> {
  /**
   * Model to use for shot change detection.
   * Supported values: "builtin/stable" (the default if unset) and
   * "builtin/latest".
   *
   * @generated from field: string model = 1;
   */
  model = "";

  constructor(data?: PartialMessage<ShotChangeDetectionConfig>) {
    super();
    proto3.util.initPartial(data, this);
  }

  static readonly runtime: typeof proto3 = proto3;
  static readonly typeName = "google.cloud.videointelligence.v1beta2.ShotChangeDetectionConfig";
  static readonly fields: FieldList = proto3.util.newFieldList(() => [
    { no: 1, name: "model", kind: "scalar", T: 9 /* ScalarType.STRING */ },
  ]);

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): ShotChangeDetectionConfig {
    return new ShotChangeDetectionConfig().fromBinary(bytes, options);
  }

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): ShotChangeDetectionConfig {
    return new ShotChangeDetectionConfig().fromJson(jsonValue, options);
  }

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): ShotChangeDetectionConfig {
    return new ShotChangeDetectionConfig().fromJsonString(jsonString, options);
  }

  static equals(a: ShotChangeDetectionConfig | PlainMessage<ShotChangeDetectionConfig> | undefined, b: ShotChangeDetectionConfig | PlainMessage<ShotChangeDetectionConfig> | undefined): boolean {
    return proto3.util.equals(ShotChangeDetectionConfig, a, b);
  }
}

/**
 * Config for EXPLICIT_CONTENT_DETECTION.
 *
 * @generated from message google.cloud.videointelligence.v1beta2.ExplicitContentDetectionConfig
 */
export class ExplicitContentDetectionConfig extends Message<ExplicitContentDetectionConfig> {
  /**
   * Model to use for explicit content detection.
   * Supported values: "builtin/stable" (the default if unset) and
   * "builtin/latest".
   *
   * @generated from field: string model = 1;
   */
  model = "";

  constructor(data?: PartialMessage<ExplicitContentDetectionConfig>) {
    super();
    proto3.util.initPartial(data, this);
  }

  static readonly runtime: typeof proto3 = proto3;
  static readonly typeName = "google.cloud.videointelligence.v1beta2.ExplicitContentDetectionConfig";
  static readonly fields: FieldList = proto3.util.newFieldList(() => [
    { no: 1, name: "model", kind: "scalar", T: 9 /* ScalarType.STRING */ },
  ]);

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): ExplicitContentDetectionConfig {
    return new ExplicitContentDetectionConfig().fromBinary(bytes, options);
  }

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): ExplicitContentDetectionConfig {
    return new ExplicitContentDetectionConfig().fromJson(jsonValue, options);
  }

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): ExplicitContentDetectionConfig {
    return new ExplicitContentDetectionConfig().fromJsonString(jsonString, options);
  }

  static equals(a: ExplicitContentDetectionConfig | PlainMessage<ExplicitContentDetectionConfig> | undefined, b: ExplicitContentDetectionConfig | PlainMessage<ExplicitContentDetectionConfig> | undefined): boolean {
    return proto3.util.equals(ExplicitContentDetectionConfig, a, b);
  }
}

/**
 * Config for FACE_DETECTION.
 *
 * @generated from message google.cloud.videointelligence.v1beta2.FaceDetectionConfig
 */
export class FaceDetectionConfig extends Message<FaceDetectionConfig> {
  /**
   * Model to use for face detection.
   * Supported values: "builtin/stable" (the default if unset) and
   * "builtin/latest".
   *
   * @generated from field: string model = 1;
   */
  model = "";

  /**
   * Whether bounding boxes be included in the face annotation output.
   *
   * @generated from field: bool include_bounding_boxes = 2;
   */
  includeBoundingBoxes = false;

  constructor(data?: PartialMessage<FaceDetectionConfig>) {
    super();
    proto3.util.initPartial(data, this);
  }

  static readonly runtime: typeof proto3 = proto3;
  static readonly typeName = "google.cloud.videointelligence.v1beta2.FaceDetectionConfig";
  static readonly fields: FieldList = proto3.util.newFieldList(() => [
    { no: 1, name: "model", kind: "scalar", T: 9 /* ScalarType.STRING */ },
    { no: 2, name: "include_bounding_boxes", kind: "scalar", T: 8 /* ScalarType.BOOL */ },
  ]);

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): FaceDetectionConfig {
    return new FaceDetectionConfig().fromBinary(bytes, options);
  }

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): FaceDetectionConfig {
    return new FaceDetectionConfig().fromJson(jsonValue, options);
  }

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): FaceDetectionConfig {
    return new FaceDetectionConfig().fromJsonString(jsonString, options);
  }

  static equals(a: FaceDetectionConfig | PlainMessage<FaceDetectionConfig> | undefined, b: FaceDetectionConfig | PlainMessage<FaceDetectionConfig> | undefined): boolean {
    return proto3.util.equals(FaceDetectionConfig, a, b);
  }
}

/**
 * Video segment.
 *
 * @generated from message google.cloud.videointelligence.v1beta2.VideoSegment
 */
export class VideoSegment extends Message<VideoSegment> {
  /**
   * Time-offset, relative to the beginning of the video,
   * corresponding to the start of the segment (inclusive).
   *
   * @generated from field: google.protobuf.Duration start_time_offset = 1;
   */
  startTimeOffset?: Duration;

  /**
   * Time-offset, relative to the beginning of the video,
   * corresponding to the end of the segment (inclusive).
   *
   * @generated from field: google.protobuf.Duration end_time_offset = 2;
   */
  endTimeOffset?: Duration;

  constructor(data?: PartialMessage<VideoSegment>) {
    super();
    proto3.util.initPartial(data, this);
  }

  static readonly runtime: typeof proto3 = proto3;
  static readonly typeName = "google.cloud.videointelligence.v1beta2.VideoSegment";
  static readonly fields: FieldList = proto3.util.newFieldList(() => [
    { no: 1, name: "start_time_offset", kind: "message", T: Duration },
    { no: 2, name: "end_time_offset", kind: "message", T: Duration },
  ]);

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): VideoSegment {
    return new VideoSegment().fromBinary(bytes, options);
  }

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): VideoSegment {
    return new VideoSegment().fromJson(jsonValue, options);
  }

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): VideoSegment {
    return new VideoSegment().fromJsonString(jsonString, options);
  }

  static equals(a: VideoSegment | PlainMessage<VideoSegment> | undefined, b: VideoSegment | PlainMessage<VideoSegment> | undefined): boolean {
    return proto3.util.equals(VideoSegment, a, b);
  }
}

/**
 * Video segment level annotation results for label detection.
 *
 * @generated from message google.cloud.videointelligence.v1beta2.LabelSegment
 */
export class LabelSegment extends Message<LabelSegment> {
  /**
   * Video segment where a label was detected.
   *
   * @generated from field: google.cloud.videointelligence.v1beta2.VideoSegment segment = 1;
   */
  segment?: VideoSegment;

  /**
   * Confidence that the label is accurate. Range: [0, 1].
   *
   * @generated from field: float confidence = 2;
   */
  confidence = 0;

  constructor(data?: PartialMessage<LabelSegment>) {
    super();
    proto3.util.initPartial(data, this);
  }

  static readonly runtime: typeof proto3 = proto3;
  static readonly typeName = "google.cloud.videointelligence.v1beta2.LabelSegment";
  static readonly fields: FieldList = proto3.util.newFieldList(() => [
    { no: 1, name: "segment", kind: "message", T: VideoSegment },
    { no: 2, name: "confidence", kind: "scalar", T: 2 /* ScalarType.FLOAT */ },
  ]);

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): LabelSegment {
    return new LabelSegment().fromBinary(bytes, options);
  }

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): LabelSegment {
    return new LabelSegment().fromJson(jsonValue, options);
  }

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): LabelSegment {
    return new LabelSegment().fromJsonString(jsonString, options);
  }

  static equals(a: LabelSegment | PlainMessage<LabelSegment> | undefined, b: LabelSegment | PlainMessage<LabelSegment> | undefined): boolean {
    return proto3.util.equals(LabelSegment, a, b);
  }
}

/**
 * Video frame level annotation results for label detection.
 *
 * @generated from message google.cloud.videointelligence.v1beta2.LabelFrame
 */
export class LabelFrame extends Message<LabelFrame> {
  /**
   * Time-offset, relative to the beginning of the video, corresponding to the
   * video frame for this location.
   *
   * @generated from field: google.protobuf.Duration time_offset = 1;
   */
  timeOffset?: Duration;

  /**
   * Confidence that the label is accurate. Range: [0, 1].
   *
   * @generated from field: float confidence = 2;
   */
  confidence = 0;

  constructor(data?: PartialMessage<LabelFrame>) {
    super();
    proto3.util.initPartial(data, this);
  }

  static readonly runtime: typeof proto3 = proto3;
  static readonly typeName = "google.cloud.videointelligence.v1beta2.LabelFrame";
  static readonly fields: FieldList = proto3.util.newFieldList(() => [
    { no: 1, name: "time_offset", kind: "message", T: Duration },
    { no: 2, name: "confidence", kind: "scalar", T: 2 /* ScalarType.FLOAT */ },
  ]);

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): LabelFrame {
    return new LabelFrame().fromBinary(bytes, options);
  }

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): LabelFrame {
    return new LabelFrame().fromJson(jsonValue, options);
  }

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): LabelFrame {
    return new LabelFrame().fromJsonString(jsonString, options);
  }

  static equals(a: LabelFrame | PlainMessage<LabelFrame> | undefined, b: LabelFrame | PlainMessage<LabelFrame> | undefined): boolean {
    return proto3.util.equals(LabelFrame, a, b);
  }
}

/**
 * Detected entity from video analysis.
 *
 * @generated from message google.cloud.videointelligence.v1beta2.Entity
 */
export class Entity extends Message<Entity> {
  /**
   * Opaque entity ID. Some IDs may be available in
   * [Google Knowledge Graph Search
   * API](https://developers.google.com/knowledge-graph/).
   *
   * @generated from field: string entity_id = 1;
   */
  entityId = "";

  /**
   * Textual description, e.g. `Fixed-gear bicycle`.
   *
   * @generated from field: string description = 2;
   */
  description = "";

  /**
   * Language code for `description` in BCP-47 format.
   *
   * @generated from field: string language_code = 3;
   */
  languageCode = "";

  constructor(data?: PartialMessage<Entity>) {
    super();
    proto3.util.initPartial(data, this);
  }

  static readonly runtime: typeof proto3 = proto3;
  static readonly typeName = "google.cloud.videointelligence.v1beta2.Entity";
  static readonly fields: FieldList = proto3.util.newFieldList(() => [
    { no: 1, name: "entity_id", kind: "scalar", T: 9 /* ScalarType.STRING */ },
    { no: 2, name: "description", kind: "scalar", T: 9 /* ScalarType.STRING */ },
    { no: 3, name: "language_code", kind: "scalar", T: 9 /* ScalarType.STRING */ },
  ]);

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): Entity {
    return new Entity().fromBinary(bytes, options);
  }

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): Entity {
    return new Entity().fromJson(jsonValue, options);
  }

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): Entity {
    return new Entity().fromJsonString(jsonString, options);
  }

  static equals(a: Entity | PlainMessage<Entity> | undefined, b: Entity | PlainMessage<Entity> | undefined): boolean {
    return proto3.util.equals(Entity, a, b);
  }
}

/**
 * Label annotation.
 *
 * @generated from message google.cloud.videointelligence.v1beta2.LabelAnnotation
 */
export class LabelAnnotation extends Message<LabelAnnotation> {
  /**
   * Detected entity.
   *
   * @generated from field: google.cloud.videointelligence.v1beta2.Entity entity = 1;
   */
  entity?: Entity;

  /**
   * Common categories for the detected entity.
   * E.g. when the label is `Terrier` the category is likely `dog`. And in some
   * cases there might be more than one categories e.g. `Terrier` could also be
   * a `pet`.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1beta2.Entity category_entities = 2;
   */
  categoryEntities: Entity[] = [];

  /**
   * All video segments where a label was detected.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1beta2.LabelSegment segments = 3;
   */
  segments: LabelSegment[] = [];

  /**
   * All video frames where a label was detected.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1beta2.LabelFrame frames = 4;
   */
  frames: LabelFrame[] = [];

  constructor(data?: PartialMessage<LabelAnnotation>) {
    super();
    proto3.util.initPartial(data, this);
  }

  static readonly runtime: typeof proto3 = proto3;
  static readonly typeName = "google.cloud.videointelligence.v1beta2.LabelAnnotation";
  static readonly fields: FieldList = proto3.util.newFieldList(() => [
    { no: 1, name: "entity", kind: "message", T: Entity },
    { no: 2, name: "category_entities", kind: "message", T: Entity, repeated: true },
    { no: 3, name: "segments", kind: "message", T: LabelSegment, repeated: true },
    { no: 4, name: "frames", kind: "message", T: LabelFrame, repeated: true },
  ]);

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): LabelAnnotation {
    return new LabelAnnotation().fromBinary(bytes, options);
  }

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): LabelAnnotation {
    return new LabelAnnotation().fromJson(jsonValue, options);
  }

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): LabelAnnotation {
    return new LabelAnnotation().fromJsonString(jsonString, options);
  }

  static equals(a: LabelAnnotation | PlainMessage<LabelAnnotation> | undefined, b: LabelAnnotation | PlainMessage<LabelAnnotation> | undefined): boolean {
    return proto3.util.equals(LabelAnnotation, a, b);
  }
}

/**
 * Video frame level annotation results for explicit content.
 *
 * @generated from message google.cloud.videointelligence.v1beta2.ExplicitContentFrame
 */
export class ExplicitContentFrame extends Message<ExplicitContentFrame> {
  /**
   * Time-offset, relative to the beginning of the video, corresponding to the
   * video frame for this location.
   *
   * @generated from field: google.protobuf.Duration time_offset = 1;
   */
  timeOffset?: Duration;

  /**
   * Likelihood of the pornography content..
   *
   * @generated from field: google.cloud.videointelligence.v1beta2.Likelihood pornography_likelihood = 2;
   */
  pornographyLikelihood = Likelihood.LIKELIHOOD_UNSPECIFIED;

  constructor(data?: PartialMessage<ExplicitContentFrame>) {
    super();
    proto3.util.initPartial(data, this);
  }

  static readonly runtime: typeof proto3 = proto3;
  static readonly typeName = "google.cloud.videointelligence.v1beta2.ExplicitContentFrame";
  static readonly fields: FieldList = proto3.util.newFieldList(() => [
    { no: 1, name: "time_offset", kind: "message", T: Duration },
    { no: 2, name: "pornography_likelihood", kind: "enum", T: proto3.getEnumType(Likelihood) },
  ]);

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): ExplicitContentFrame {
    return new ExplicitContentFrame().fromBinary(bytes, options);
  }

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): ExplicitContentFrame {
    return new ExplicitContentFrame().fromJson(jsonValue, options);
  }

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): ExplicitContentFrame {
    return new ExplicitContentFrame().fromJsonString(jsonString, options);
  }

  static equals(a: ExplicitContentFrame | PlainMessage<ExplicitContentFrame> | undefined, b: ExplicitContentFrame | PlainMessage<ExplicitContentFrame> | undefined): boolean {
    return proto3.util.equals(ExplicitContentFrame, a, b);
  }
}

/**
 * Explicit content annotation (based on per-frame visual signals only).
 * If no explicit content has been detected in a frame, no annotations are
 * present for that frame.
 *
 * @generated from message google.cloud.videointelligence.v1beta2.ExplicitContentAnnotation
 */
export class ExplicitContentAnnotation extends Message<ExplicitContentAnnotation> {
  /**
   * All video frames where explicit content was detected.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1beta2.ExplicitContentFrame frames = 1;
   */
  frames: ExplicitContentFrame[] = [];

  constructor(data?: PartialMessage<ExplicitContentAnnotation>) {
    super();
    proto3.util.initPartial(data, this);
  }

  static readonly runtime: typeof proto3 = proto3;
  static readonly typeName = "google.cloud.videointelligence.v1beta2.ExplicitContentAnnotation";
  static readonly fields: FieldList = proto3.util.newFieldList(() => [
    { no: 1, name: "frames", kind: "message", T: ExplicitContentFrame, repeated: true },
  ]);

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): ExplicitContentAnnotation {
    return new ExplicitContentAnnotation().fromBinary(bytes, options);
  }

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): ExplicitContentAnnotation {
    return new ExplicitContentAnnotation().fromJson(jsonValue, options);
  }

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): ExplicitContentAnnotation {
    return new ExplicitContentAnnotation().fromJsonString(jsonString, options);
  }

  static equals(a: ExplicitContentAnnotation | PlainMessage<ExplicitContentAnnotation> | undefined, b: ExplicitContentAnnotation | PlainMessage<ExplicitContentAnnotation> | undefined): boolean {
    return proto3.util.equals(ExplicitContentAnnotation, a, b);
  }
}

/**
 * Normalized bounding box.
 * The normalized vertex coordinates are relative to the original image.
 * Range: [0, 1].
 *
 * @generated from message google.cloud.videointelligence.v1beta2.NormalizedBoundingBox
 */
export class NormalizedBoundingBox extends Message<NormalizedBoundingBox> {
  /**
   * Left X coordinate.
   *
   * @generated from field: float left = 1;
   */
  left = 0;

  /**
   * Top Y coordinate.
   *
   * @generated from field: float top = 2;
   */
  top = 0;

  /**
   * Right X coordinate.
   *
   * @generated from field: float right = 3;
   */
  right = 0;

  /**
   * Bottom Y coordinate.
   *
   * @generated from field: float bottom = 4;
   */
  bottom = 0;

  constructor(data?: PartialMessage<NormalizedBoundingBox>) {
    super();
    proto3.util.initPartial(data, this);
  }

  static readonly runtime: typeof proto3 = proto3;
  static readonly typeName = "google.cloud.videointelligence.v1beta2.NormalizedBoundingBox";
  static readonly fields: FieldList = proto3.util.newFieldList(() => [
    { no: 1, name: "left", kind: "scalar", T: 2 /* ScalarType.FLOAT */ },
    { no: 2, name: "top", kind: "scalar", T: 2 /* ScalarType.FLOAT */ },
    { no: 3, name: "right", kind: "scalar", T: 2 /* ScalarType.FLOAT */ },
    { no: 4, name: "bottom", kind: "scalar", T: 2 /* ScalarType.FLOAT */ },
  ]);

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): NormalizedBoundingBox {
    return new NormalizedBoundingBox().fromBinary(bytes, options);
  }

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): NormalizedBoundingBox {
    return new NormalizedBoundingBox().fromJson(jsonValue, options);
  }

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): NormalizedBoundingBox {
    return new NormalizedBoundingBox().fromJsonString(jsonString, options);
  }

  static equals(a: NormalizedBoundingBox | PlainMessage<NormalizedBoundingBox> | undefined, b: NormalizedBoundingBox | PlainMessage<NormalizedBoundingBox> | undefined): boolean {
    return proto3.util.equals(NormalizedBoundingBox, a, b);
  }
}

/**
 * Video segment level annotation results for face detection.
 *
 * @generated from message google.cloud.videointelligence.v1beta2.FaceSegment
 */
export class FaceSegment extends Message<FaceSegment> {
  /**
   * Video segment where a face was detected.
   *
   * @generated from field: google.cloud.videointelligence.v1beta2.VideoSegment segment = 1;
   */
  segment?: VideoSegment;

  constructor(data?: PartialMessage<FaceSegment>) {
    super();
    proto3.util.initPartial(data, this);
  }

  static readonly runtime: typeof proto3 = proto3;
  static readonly typeName = "google.cloud.videointelligence.v1beta2.FaceSegment";
  static readonly fields: FieldList = proto3.util.newFieldList(() => [
    { no: 1, name: "segment", kind: "message", T: VideoSegment },
  ]);

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): FaceSegment {
    return new FaceSegment().fromBinary(bytes, options);
  }

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): FaceSegment {
    return new FaceSegment().fromJson(jsonValue, options);
  }

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): FaceSegment {
    return new FaceSegment().fromJsonString(jsonString, options);
  }

  static equals(a: FaceSegment | PlainMessage<FaceSegment> | undefined, b: FaceSegment | PlainMessage<FaceSegment> | undefined): boolean {
    return proto3.util.equals(FaceSegment, a, b);
  }
}

/**
 * Video frame level annotation results for face detection.
 *
 * @generated from message google.cloud.videointelligence.v1beta2.FaceFrame
 */
export class FaceFrame extends Message<FaceFrame> {
  /**
   * Normalized Bounding boxes in a frame.
   * There can be more than one boxes if the same face is detected in multiple
   * locations within the current frame.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1beta2.NormalizedBoundingBox normalized_bounding_boxes = 1;
   */
  normalizedBoundingBoxes: NormalizedBoundingBox[] = [];

  /**
   * Time-offset, relative to the beginning of the video,
   * corresponding to the video frame for this location.
   *
   * @generated from field: google.protobuf.Duration time_offset = 2;
   */
  timeOffset?: Duration;

  constructor(data?: PartialMessage<FaceFrame>) {
    super();
    proto3.util.initPartial(data, this);
  }

  static readonly runtime: typeof proto3 = proto3;
  static readonly typeName = "google.cloud.videointelligence.v1beta2.FaceFrame";
  static readonly fields: FieldList = proto3.util.newFieldList(() => [
    { no: 1, name: "normalized_bounding_boxes", kind: "message", T: NormalizedBoundingBox, repeated: true },
    { no: 2, name: "time_offset", kind: "message", T: Duration },
  ]);

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): FaceFrame {
    return new FaceFrame().fromBinary(bytes, options);
  }

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): FaceFrame {
    return new FaceFrame().fromJson(jsonValue, options);
  }

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): FaceFrame {
    return new FaceFrame().fromJsonString(jsonString, options);
  }

  static equals(a: FaceFrame | PlainMessage<FaceFrame> | undefined, b: FaceFrame | PlainMessage<FaceFrame> | undefined): boolean {
    return proto3.util.equals(FaceFrame, a, b);
  }
}

/**
 * Face annotation.
 *
 * @generated from message google.cloud.videointelligence.v1beta2.FaceAnnotation
 */
export class FaceAnnotation extends Message<FaceAnnotation> {
  /**
   * Thumbnail of a representative face view (in JPEG format).
   *
   * @generated from field: bytes thumbnail = 1;
   */
  thumbnail = new Uint8Array(0);

  /**
   * All video segments where a face was detected.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1beta2.FaceSegment segments = 2;
   */
  segments: FaceSegment[] = [];

  /**
   * All video frames where a face was detected.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1beta2.FaceFrame frames = 3;
   */
  frames: FaceFrame[] = [];

  constructor(data?: PartialMessage<FaceAnnotation>) {
    super();
    proto3.util.initPartial(data, this);
  }

  static readonly runtime: typeof proto3 = proto3;
  static readonly typeName = "google.cloud.videointelligence.v1beta2.FaceAnnotation";
  static readonly fields: FieldList = proto3.util.newFieldList(() => [
    { no: 1, name: "thumbnail", kind: "scalar", T: 12 /* ScalarType.BYTES */ },
    { no: 2, name: "segments", kind: "message", T: FaceSegment, repeated: true },
    { no: 3, name: "frames", kind: "message", T: FaceFrame, repeated: true },
  ]);

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): FaceAnnotation {
    return new FaceAnnotation().fromBinary(bytes, options);
  }

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): FaceAnnotation {
    return new FaceAnnotation().fromJson(jsonValue, options);
  }

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): FaceAnnotation {
    return new FaceAnnotation().fromJsonString(jsonString, options);
  }

  static equals(a: FaceAnnotation | PlainMessage<FaceAnnotation> | undefined, b: FaceAnnotation | PlainMessage<FaceAnnotation> | undefined): boolean {
    return proto3.util.equals(FaceAnnotation, a, b);
  }
}

/**
 * Annotation results for a single video.
 *
 * @generated from message google.cloud.videointelligence.v1beta2.VideoAnnotationResults
 */
export class VideoAnnotationResults extends Message<VideoAnnotationResults> {
  /**
   * Video file location in
   * [Google Cloud Storage](https://cloud.google.com/storage/).
   *
   * @generated from field: string input_uri = 1;
   */
  inputUri = "";

  /**
   * Label annotations on video level or user specified segment level.
   * There is exactly one element for each unique label.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1beta2.LabelAnnotation segment_label_annotations = 2;
   */
  segmentLabelAnnotations: LabelAnnotation[] = [];

  /**
   * Label annotations on shot level.
   * There is exactly one element for each unique label.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1beta2.LabelAnnotation shot_label_annotations = 3;
   */
  shotLabelAnnotations: LabelAnnotation[] = [];

  /**
   * Label annotations on frame level.
   * There is exactly one element for each unique label.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1beta2.LabelAnnotation frame_label_annotations = 4;
   */
  frameLabelAnnotations: LabelAnnotation[] = [];

  /**
   * Face annotations. There is exactly one element for each unique face.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1beta2.FaceAnnotation face_annotations = 5;
   */
  faceAnnotations: FaceAnnotation[] = [];

  /**
   * Shot annotations. Each shot is represented as a video segment.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1beta2.VideoSegment shot_annotations = 6;
   */
  shotAnnotations: VideoSegment[] = [];

  /**
   * Explicit content annotation.
   *
   * @generated from field: google.cloud.videointelligence.v1beta2.ExplicitContentAnnotation explicit_annotation = 7;
   */
  explicitAnnotation?: ExplicitContentAnnotation;

  /**
   * If set, indicates an error. Note that for a single `AnnotateVideoRequest`
   * some videos may succeed and some may fail.
   *
   * @generated from field: google.rpc.Status error = 9;
   */
  error?: Status;

  constructor(data?: PartialMessage<VideoAnnotationResults>) {
    super();
    proto3.util.initPartial(data, this);
  }

  static readonly runtime: typeof proto3 = proto3;
  static readonly typeName = "google.cloud.videointelligence.v1beta2.VideoAnnotationResults";
  static readonly fields: FieldList = proto3.util.newFieldList(() => [
    { no: 1, name: "input_uri", kind: "scalar", T: 9 /* ScalarType.STRING */ },
    { no: 2, name: "segment_label_annotations", kind: "message", T: LabelAnnotation, repeated: true },
    { no: 3, name: "shot_label_annotations", kind: "message", T: LabelAnnotation, repeated: true },
    { no: 4, name: "frame_label_annotations", kind: "message", T: LabelAnnotation, repeated: true },
    { no: 5, name: "face_annotations", kind: "message", T: FaceAnnotation, repeated: true },
    { no: 6, name: "shot_annotations", kind: "message", T: VideoSegment, repeated: true },
    { no: 7, name: "explicit_annotation", kind: "message", T: ExplicitContentAnnotation },
    { no: 9, name: "error", kind: "message", T: Status },
  ]);

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): VideoAnnotationResults {
    return new VideoAnnotationResults().fromBinary(bytes, options);
  }

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): VideoAnnotationResults {
    return new VideoAnnotationResults().fromJson(jsonValue, options);
  }

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): VideoAnnotationResults {
    return new VideoAnnotationResults().fromJsonString(jsonString, options);
  }

  static equals(a: VideoAnnotationResults | PlainMessage<VideoAnnotationResults> | undefined, b: VideoAnnotationResults | PlainMessage<VideoAnnotationResults> | undefined): boolean {
    return proto3.util.equals(VideoAnnotationResults, a, b);
  }
}

/**
 * Video annotation response. Included in the `response`
 * field of the `Operation` returned by the `GetOperation`
 * call of the `google::longrunning::Operations` service.
 *
 * @generated from message google.cloud.videointelligence.v1beta2.AnnotateVideoResponse
 */
export class AnnotateVideoResponse extends Message<AnnotateVideoResponse> {
  /**
   * Annotation results for all videos specified in `AnnotateVideoRequest`.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1beta2.VideoAnnotationResults annotation_results = 1;
   */
  annotationResults: VideoAnnotationResults[] = [];

  constructor(data?: PartialMessage<AnnotateVideoResponse>) {
    super();
    proto3.util.initPartial(data, this);
  }

  static readonly runtime: typeof proto3 = proto3;
  static readonly typeName = "google.cloud.videointelligence.v1beta2.AnnotateVideoResponse";
  static readonly fields: FieldList = proto3.util.newFieldList(() => [
    { no: 1, name: "annotation_results", kind: "message", T: VideoAnnotationResults, repeated: true },
  ]);

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): AnnotateVideoResponse {
    return new AnnotateVideoResponse().fromBinary(bytes, options);
  }

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): AnnotateVideoResponse {
    return new AnnotateVideoResponse().fromJson(jsonValue, options);
  }

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): AnnotateVideoResponse {
    return new AnnotateVideoResponse().fromJsonString(jsonString, options);
  }

  static equals(a: AnnotateVideoResponse | PlainMessage<AnnotateVideoResponse> | undefined, b: AnnotateVideoResponse | PlainMessage<AnnotateVideoResponse> | undefined): boolean {
    return proto3.util.equals(AnnotateVideoResponse, a, b);
  }
}

/**
 * Annotation progress for a single video.
 *
 * @generated from message google.cloud.videointelligence.v1beta2.VideoAnnotationProgress
 */
export class VideoAnnotationProgress extends Message<VideoAnnotationProgress> {
  /**
   * Video file location in
   * [Google Cloud Storage](https://cloud.google.com/storage/).
   *
   * @generated from field: string input_uri = 1;
   */
  inputUri = "";

  /**
   * Approximate percentage processed thus far.
   * Guaranteed to be 100 when fully processed.
   *
   * @generated from field: int32 progress_percent = 2;
   */
  progressPercent = 0;

  /**
   * Time when the request was received.
   *
   * @generated from field: google.protobuf.Timestamp start_time = 3;
   */
  startTime?: Timestamp;

  /**
   * Time of the most recent update.
   *
   * @generated from field: google.protobuf.Timestamp update_time = 4;
   */
  updateTime?: Timestamp;

  constructor(data?: PartialMessage<VideoAnnotationProgress>) {
    super();
    proto3.util.initPartial(data, this);
  }

  static readonly runtime: typeof proto3 = proto3;
  static readonly typeName = "google.cloud.videointelligence.v1beta2.VideoAnnotationProgress";
  static readonly fields: FieldList = proto3.util.newFieldList(() => [
    { no: 1, name: "input_uri", kind: "scalar", T: 9 /* ScalarType.STRING */ },
    { no: 2, name: "progress_percent", kind: "scalar", T: 5 /* ScalarType.INT32 */ },
    { no: 3, name: "start_time", kind: "message", T: Timestamp },
    { no: 4, name: "update_time", kind: "message", T: Timestamp },
  ]);

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): VideoAnnotationProgress {
    return new VideoAnnotationProgress().fromBinary(bytes, options);
  }

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): VideoAnnotationProgress {
    return new VideoAnnotationProgress().fromJson(jsonValue, options);
  }

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): VideoAnnotationProgress {
    return new VideoAnnotationProgress().fromJsonString(jsonString, options);
  }

  static equals(a: VideoAnnotationProgress | PlainMessage<VideoAnnotationProgress> | undefined, b: VideoAnnotationProgress | PlainMessage<VideoAnnotationProgress> | undefined): boolean {
    return proto3.util.equals(VideoAnnotationProgress, a, b);
  }
}

/**
 * Video annotation progress. Included in the `metadata`
 * field of the `Operation` returned by the `GetOperation`
 * call of the `google::longrunning::Operations` service.
 *
 * @generated from message google.cloud.videointelligence.v1beta2.AnnotateVideoProgress
 */
export class AnnotateVideoProgress extends Message<AnnotateVideoProgress> {
  /**
   * Progress metadata for all videos specified in `AnnotateVideoRequest`.
   *
   * @generated from field: repeated google.cloud.videointelligence.v1beta2.VideoAnnotationProgress annotation_progress = 1;
   */
  annotationProgress: VideoAnnotationProgress[] = [];

  constructor(data?: PartialMessage<AnnotateVideoProgress>) {
    super();
    proto3.util.initPartial(data, this);
  }

  static readonly runtime: typeof proto3 = proto3;
  static readonly typeName = "google.cloud.videointelligence.v1beta2.AnnotateVideoProgress";
  static readonly fields: FieldList = proto3.util.newFieldList(() => [
    { no: 1, name: "annotation_progress", kind: "message", T: VideoAnnotationProgress, repeated: true },
  ]);

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): AnnotateVideoProgress {
    return new AnnotateVideoProgress().fromBinary(bytes, options);
  }

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): AnnotateVideoProgress {
    return new AnnotateVideoProgress().fromJson(jsonValue, options);
  }

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): AnnotateVideoProgress {
    return new AnnotateVideoProgress().fromJsonString(jsonString, options);
  }

  static equals(a: AnnotateVideoProgress | PlainMessage<AnnotateVideoProgress> | undefined, b: AnnotateVideoProgress | PlainMessage<AnnotateVideoProgress> | undefined): boolean {
    return proto3.util.equals(AnnotateVideoProgress, a, b);
  }
}


// Copyright 2016 Google Inc.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

// @generated by protoc-gen-es v1.3.0 with parameter "target=ts"
// @generated from file google/cloud/dataproc/v1/jobs.proto (package google.cloud.dataproc.v1, syntax proto3)
/* eslint-disable */
// @ts-nocheck

import type { BinaryReadOptions, FieldList, JsonReadOptions, JsonValue, PartialMessage, PlainMessage } from "@bufbuild/protobuf";
import { Message, proto3, Timestamp } from "@bufbuild/protobuf";

/**
 * The runtime logging config of the job.
 *
 * @generated from message google.cloud.dataproc.v1.LoggingConfig
 */
export class LoggingConfig extends Message<LoggingConfig> {
  /**
   * The per-package log levels for the driver. This may include
   * "root" package name to configure rootLogger.
   * Examples:
   *   'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
   *
   * @generated from field: map<string, google.cloud.dataproc.v1.LoggingConfig.Level> driver_log_levels = 2;
   */
  driverLogLevels: { [key: string]: LoggingConfig_Level } = {};

  constructor(data?: PartialMessage<LoggingConfig>) {
    super();
    proto3.util.initPartial(data, this);
  }

  static readonly runtime: typeof proto3 = proto3;
  static readonly typeName = "google.cloud.dataproc.v1.LoggingConfig";
  static readonly fields: FieldList = proto3.util.newFieldList(() => [
    { no: 2, name: "driver_log_levels", kind: "map", K: 9 /* ScalarType.STRING */, V: {kind: "enum", T: proto3.getEnumType(LoggingConfig_Level)} },
  ]);

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): LoggingConfig {
    return new LoggingConfig().fromBinary(bytes, options);
  }

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): LoggingConfig {
    return new LoggingConfig().fromJson(jsonValue, options);
  }

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): LoggingConfig {
    return new LoggingConfig().fromJsonString(jsonString, options);
  }

  static equals(a: LoggingConfig | PlainMessage<LoggingConfig> | undefined, b: LoggingConfig | PlainMessage<LoggingConfig> | undefined): boolean {
    return proto3.util.equals(LoggingConfig, a, b);
  }
}

/**
 * The Log4j level for job execution. When running an
 * [Apache Hive](http://hive.apache.org/) job, Cloud
 * Dataproc configures the Hive client to an equivalent verbosity level.
 *
 * @generated from enum google.cloud.dataproc.v1.LoggingConfig.Level
 */
export enum LoggingConfig_Level {
  /**
   * Level is unspecified. Use default level for log4j.
   *
   * @generated from enum value: LEVEL_UNSPECIFIED = 0;
   */
  LEVEL_UNSPECIFIED = 0,

  /**
   * Use ALL level for log4j.
   *
   * @generated from enum value: ALL = 1;
   */
  ALL = 1,

  /**
   * Use TRACE level for log4j.
   *
   * @generated from enum value: TRACE = 2;
   */
  TRACE = 2,

  /**
   * Use DEBUG level for log4j.
   *
   * @generated from enum value: DEBUG = 3;
   */
  DEBUG = 3,

  /**
   * Use INFO level for log4j.
   *
   * @generated from enum value: INFO = 4;
   */
  INFO = 4,

  /**
   * Use WARN level for log4j.
   *
   * @generated from enum value: WARN = 5;
   */
  WARN = 5,

  /**
   * Use ERROR level for log4j.
   *
   * @generated from enum value: ERROR = 6;
   */
  ERROR = 6,

  /**
   * Use FATAL level for log4j.
   *
   * @generated from enum value: FATAL = 7;
   */
  FATAL = 7,

  /**
   * Turn off log4j.
   *
   * @generated from enum value: OFF = 8;
   */
  OFF = 8,
}
// Retrieve enum metadata with: proto3.getEnumType(LoggingConfig_Level)
proto3.util.setEnumType(LoggingConfig_Level, "google.cloud.dataproc.v1.LoggingConfig.Level", [
  { no: 0, name: "LEVEL_UNSPECIFIED" },
  { no: 1, name: "ALL" },
  { no: 2, name: "TRACE" },
  { no: 3, name: "DEBUG" },
  { no: 4, name: "INFO" },
  { no: 5, name: "WARN" },
  { no: 6, name: "ERROR" },
  { no: 7, name: "FATAL" },
  { no: 8, name: "OFF" },
]);

/**
 * A Cloud Dataproc job for running
 * [Apache Hadoop MapReduce](https://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html)
 * jobs on [Apache Hadoop YARN](https://hadoop.apache.org/docs/r2.7.1/hadoop-yarn/hadoop-yarn-site/YARN.html).
 *
 * @generated from message google.cloud.dataproc.v1.HadoopJob
 */
export class HadoopJob extends Message<HadoopJob> {
  /**
   * [Required] Indicates the location of the driver's main class. Specify
   * either the jar file that contains the main class or the main class name.
   * To specify both, add the jar file to `jar_file_uris`, and then specify
   * the main class name in this property.
   *
   * @generated from oneof google.cloud.dataproc.v1.HadoopJob.driver
   */
  driver: {
    /**
     * The HCFS URI of the jar file containing the main class.
     * Examples:
     *     'gs://foo-bucket/analytics-binaries/extract-useful-metrics-mr.jar'
     *     'hdfs:/tmp/test-samples/custom-wordcount.jar'
     *     'file:///home/usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar'
     *
     * @generated from field: string main_jar_file_uri = 1;
     */
    value: string;
    case: "mainJarFileUri";
  } | {
    /**
     * The name of the driver's main class. The jar file containing the class
     * must be in the default CLASSPATH or specified in `jar_file_uris`.
     *
     * @generated from field: string main_class = 2;
     */
    value: string;
    case: "mainClass";
  } | { case: undefined; value?: undefined } = { case: undefined };

  /**
   * [Optional] The arguments to pass to the driver. Do not
   * include arguments, such as `-libjars` or `-Dfoo=bar`, that can be set as job
   * properties, since a collision may occur that causes an incorrect job
   * submission.
   *
   * @generated from field: repeated string args = 3;
   */
  args: string[] = [];

  /**
   * [Optional] Jar file URIs to add to the CLASSPATHs of the
   * Hadoop driver and tasks.
   *
   * @generated from field: repeated string jar_file_uris = 4;
   */
  jarFileUris: string[] = [];

  /**
   * [Optional] HCFS (Hadoop Compatible Filesystem) URIs of files to be copied
   * to the working directory of Hadoop drivers and distributed tasks. Useful
   * for naively parallel tasks.
   *
   * @generated from field: repeated string file_uris = 5;
   */
  fileUris: string[] = [];

  /**
   * [Optional] HCFS URIs of archives to be extracted in the working directory of
   * Hadoop drivers and tasks. Supported file types:
   * .jar, .tar, .tar.gz, .tgz, or .zip.
   *
   * @generated from field: repeated string archive_uris = 6;
   */
  archiveUris: string[] = [];

  /**
   * [Optional] A mapping of property names to values, used to configure Hadoop.
   * Properties that conflict with values set by the Cloud Dataproc API may be
   * overwritten. Can include properties set in /etc/hadoop/conf/*-site and
   * classes in user code.
   *
   * @generated from field: map<string, string> properties = 7;
   */
  properties: { [key: string]: string } = {};

  /**
   * [Optional] The runtime log config for job execution.
   *
   * @generated from field: google.cloud.dataproc.v1.LoggingConfig logging_config = 8;
   */
  loggingConfig?: LoggingConfig;

  constructor(data?: PartialMessage<HadoopJob>) {
    super();
    proto3.util.initPartial(data, this);
  }

  static readonly runtime: typeof proto3 = proto3;
  static readonly typeName = "google.cloud.dataproc.v1.HadoopJob";
  static readonly fields: FieldList = proto3.util.newFieldList(() => [
    { no: 1, name: "main_jar_file_uri", kind: "scalar", T: 9 /* ScalarType.STRING */, oneof: "driver" },
    { no: 2, name: "main_class", kind: "scalar", T: 9 /* ScalarType.STRING */, oneof: "driver" },
    { no: 3, name: "args", kind: "scalar", T: 9 /* ScalarType.STRING */, repeated: true },
    { no: 4, name: "jar_file_uris", kind: "scalar", T: 9 /* ScalarType.STRING */, repeated: true },
    { no: 5, name: "file_uris", kind: "scalar", T: 9 /* ScalarType.STRING */, repeated: true },
    { no: 6, name: "archive_uris", kind: "scalar", T: 9 /* ScalarType.STRING */, repeated: true },
    { no: 7, name: "properties", kind: "map", K: 9 /* ScalarType.STRING */, V: {kind: "scalar", T: 9 /* ScalarType.STRING */} },
    { no: 8, name: "logging_config", kind: "message", T: LoggingConfig },
  ]);

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): HadoopJob {
    return new HadoopJob().fromBinary(bytes, options);
  }

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): HadoopJob {
    return new HadoopJob().fromJson(jsonValue, options);
  }

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): HadoopJob {
    return new HadoopJob().fromJsonString(jsonString, options);
  }

  static equals(a: HadoopJob | PlainMessage<HadoopJob> | undefined, b: HadoopJob | PlainMessage<HadoopJob> | undefined): boolean {
    return proto3.util.equals(HadoopJob, a, b);
  }
}

/**
 * A Cloud Dataproc job for running [Apache Spark](http://spark.apache.org/)
 * applications on YARN.
 *
 * @generated from message google.cloud.dataproc.v1.SparkJob
 */
export class SparkJob extends Message<SparkJob> {
  /**
   * [Required] The specification of the main method to call to drive the job.
   * Specify either the jar file that contains the main class or the main class
   * name. To pass both a main jar and a main class in that jar, add the jar to
   * `CommonJob.jar_file_uris`, and then specify the main class name in `main_class`.
   *
   * @generated from oneof google.cloud.dataproc.v1.SparkJob.driver
   */
  driver: {
    /**
     * The HCFS URI of the jar file that contains the main class.
     *
     * @generated from field: string main_jar_file_uri = 1;
     */
    value: string;
    case: "mainJarFileUri";
  } | {
    /**
     * The name of the driver's main class. The jar file that contains the class
     * must be in the default CLASSPATH or specified in `jar_file_uris`.
     *
     * @generated from field: string main_class = 2;
     */
    value: string;
    case: "mainClass";
  } | { case: undefined; value?: undefined } = { case: undefined };

  /**
   * [Optional] The arguments to pass to the driver. Do not include arguments,
   * such as `--conf`, that can be set as job properties, since a collision may
   * occur that causes an incorrect job submission.
   *
   * @generated from field: repeated string args = 3;
   */
  args: string[] = [];

  /**
   * [Optional] HCFS URIs of jar files to add to the CLASSPATHs of the
   * Spark driver and tasks.
   *
   * @generated from field: repeated string jar_file_uris = 4;
   */
  jarFileUris: string[] = [];

  /**
   * [Optional] HCFS URIs of files to be copied to the working directory of
   * Spark drivers and distributed tasks. Useful for naively parallel tasks.
   *
   * @generated from field: repeated string file_uris = 5;
   */
  fileUris: string[] = [];

  /**
   * [Optional] HCFS URIs of archives to be extracted in the working directory
   * of Spark drivers and tasks. Supported file types:
   * .jar, .tar, .tar.gz, .tgz, and .zip.
   *
   * @generated from field: repeated string archive_uris = 6;
   */
  archiveUris: string[] = [];

  /**
   * [Optional] A mapping of property names to values, used to configure Spark.
   * Properties that conflict with values set by the Cloud Dataproc API may be
   * overwritten. Can include properties set in
   * /etc/spark/conf/spark-defaults.conf and classes in user code.
   *
   * @generated from field: map<string, string> properties = 7;
   */
  properties: { [key: string]: string } = {};

  /**
   * [Optional] The runtime log config for job execution.
   *
   * @generated from field: google.cloud.dataproc.v1.LoggingConfig logging_config = 8;
   */
  loggingConfig?: LoggingConfig;

  constructor(data?: PartialMessage<SparkJob>) {
    super();
    proto3.util.initPartial(data, this);
  }

  static readonly runtime: typeof proto3 = proto3;
  static readonly typeName = "google.cloud.dataproc.v1.SparkJob";
  static readonly fields: FieldList = proto3.util.newFieldList(() => [
    { no: 1, name: "main_jar_file_uri", kind: "scalar", T: 9 /* ScalarType.STRING */, oneof: "driver" },
    { no: 2, name: "main_class", kind: "scalar", T: 9 /* ScalarType.STRING */, oneof: "driver" },
    { no: 3, name: "args", kind: "scalar", T: 9 /* ScalarType.STRING */, repeated: true },
    { no: 4, name: "jar_file_uris", kind: "scalar", T: 9 /* ScalarType.STRING */, repeated: true },
    { no: 5, name: "file_uris", kind: "scalar", T: 9 /* ScalarType.STRING */, repeated: true },
    { no: 6, name: "archive_uris", kind: "scalar", T: 9 /* ScalarType.STRING */, repeated: true },
    { no: 7, name: "properties", kind: "map", K: 9 /* ScalarType.STRING */, V: {kind: "scalar", T: 9 /* ScalarType.STRING */} },
    { no: 8, name: "logging_config", kind: "message", T: LoggingConfig },
  ]);

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): SparkJob {
    return new SparkJob().fromBinary(bytes, options);
  }

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): SparkJob {
    return new SparkJob().fromJson(jsonValue, options);
  }

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): SparkJob {
    return new SparkJob().fromJsonString(jsonString, options);
  }

  static equals(a: SparkJob | PlainMessage<SparkJob> | undefined, b: SparkJob | PlainMessage<SparkJob> | undefined): boolean {
    return proto3.util.equals(SparkJob, a, b);
  }
}

/**
 * A Cloud Dataproc job for running
 * [Apache PySpark](https://spark.apache.org/docs/0.9.0/python-programming-guide.html)
 * applications on YARN.
 *
 * @generated from message google.cloud.dataproc.v1.PySparkJob
 */
export class PySparkJob extends Message<PySparkJob> {
  /**
   * [Required] The HCFS URI of the main Python file to use as the driver. Must
   * be a .py file.
   *
   * @generated from field: string main_python_file_uri = 1;
   */
  mainPythonFileUri = "";

  /**
   * [Optional] The arguments to pass to the driver.  Do not include arguments,
   * such as `--conf`, that can be set as job properties, since a collision may
   * occur that causes an incorrect job submission.
   *
   * @generated from field: repeated string args = 2;
   */
  args: string[] = [];

  /**
   * [Optional] HCFS file URIs of Python files to pass to the PySpark
   * framework. Supported file types: .py, .egg, and .zip.
   *
   * @generated from field: repeated string python_file_uris = 3;
   */
  pythonFileUris: string[] = [];

  /**
   * [Optional] HCFS URIs of jar files to add to the CLASSPATHs of the
   * Python driver and tasks.
   *
   * @generated from field: repeated string jar_file_uris = 4;
   */
  jarFileUris: string[] = [];

  /**
   * [Optional] HCFS URIs of files to be copied to the working directory of
   * Python drivers and distributed tasks. Useful for naively parallel tasks.
   *
   * @generated from field: repeated string file_uris = 5;
   */
  fileUris: string[] = [];

  /**
   * [Optional] HCFS URIs of archives to be extracted in the working directory of
   * .jar, .tar, .tar.gz, .tgz, and .zip.
   *
   * @generated from field: repeated string archive_uris = 6;
   */
  archiveUris: string[] = [];

  /**
   * [Optional] A mapping of property names to values, used to configure PySpark.
   * Properties that conflict with values set by the Cloud Dataproc API may be
   * overwritten. Can include properties set in
   * /etc/spark/conf/spark-defaults.conf and classes in user code.
   *
   * @generated from field: map<string, string> properties = 7;
   */
  properties: { [key: string]: string } = {};

  /**
   * [Optional] The runtime log config for job execution.
   *
   * @generated from field: google.cloud.dataproc.v1.LoggingConfig logging_config = 8;
   */
  loggingConfig?: LoggingConfig;

  constructor(data?: PartialMessage<PySparkJob>) {
    super();
    proto3.util.initPartial(data, this);
  }

  static readonly runtime: typeof proto3 = proto3;
  static readonly typeName = "google.cloud.dataproc.v1.PySparkJob";
  static readonly fields: FieldList = proto3.util.newFieldList(() => [
    { no: 1, name: "main_python_file_uri", kind: "scalar", T: 9 /* ScalarType.STRING */ },
    { no: 2, name: "args", kind: "scalar", T: 9 /* ScalarType.STRING */, repeated: true },
    { no: 3, name: "python_file_uris", kind: "scalar", T: 9 /* ScalarType.STRING */, repeated: true },
    { no: 4, name: "jar_file_uris", kind: "scalar", T: 9 /* ScalarType.STRING */, repeated: true },
    { no: 5, name: "file_uris", kind: "scalar", T: 9 /* ScalarType.STRING */, repeated: true },
    { no: 6, name: "archive_uris", kind: "scalar", T: 9 /* ScalarType.STRING */, repeated: true },
    { no: 7, name: "properties", kind: "map", K: 9 /* ScalarType.STRING */, V: {kind: "scalar", T: 9 /* ScalarType.STRING */} },
    { no: 8, name: "logging_config", kind: "message", T: LoggingConfig },
  ]);

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): PySparkJob {
    return new PySparkJob().fromBinary(bytes, options);
  }

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): PySparkJob {
    return new PySparkJob().fromJson(jsonValue, options);
  }

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): PySparkJob {
    return new PySparkJob().fromJsonString(jsonString, options);
  }

  static equals(a: PySparkJob | PlainMessage<PySparkJob> | undefined, b: PySparkJob | PlainMessage<PySparkJob> | undefined): boolean {
    return proto3.util.equals(PySparkJob, a, b);
  }
}

/**
 * A list of queries to run on a cluster.
 *
 * @generated from message google.cloud.dataproc.v1.QueryList
 */
export class QueryList extends Message<QueryList> {
  /**
   * [Required] The queries to execute. You do not need to terminate a query
   * with a semicolon. Multiple queries can be specified in one string
   * by separating each with a semicolon. Here is an example of an Cloud
   * Dataproc API snippet that uses a QueryList to specify a HiveJob:
   *
   *     "hiveJob": {
   *       "queryList": {
   *         "queries": [
   *           "query1",
   *           "query2",
   *           "query3;query4",
   *         ]
   *       }
   *     }
   *
   * @generated from field: repeated string queries = 1;
   */
  queries: string[] = [];

  constructor(data?: PartialMessage<QueryList>) {
    super();
    proto3.util.initPartial(data, this);
  }

  static readonly runtime: typeof proto3 = proto3;
  static readonly typeName = "google.cloud.dataproc.v1.QueryList";
  static readonly fields: FieldList = proto3.util.newFieldList(() => [
    { no: 1, name: "queries", kind: "scalar", T: 9 /* ScalarType.STRING */, repeated: true },
  ]);

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): QueryList {
    return new QueryList().fromBinary(bytes, options);
  }

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): QueryList {
    return new QueryList().fromJson(jsonValue, options);
  }

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): QueryList {
    return new QueryList().fromJsonString(jsonString, options);
  }

  static equals(a: QueryList | PlainMessage<QueryList> | undefined, b: QueryList | PlainMessage<QueryList> | undefined): boolean {
    return proto3.util.equals(QueryList, a, b);
  }
}

/**
 * A Cloud Dataproc job for running [Apache Hive](https://hive.apache.org/)
 * queries on YARN.
 *
 * @generated from message google.cloud.dataproc.v1.HiveJob
 */
export class HiveJob extends Message<HiveJob> {
  /**
   * [Required] The sequence of Hive queries to execute, specified as either
   * an HCFS file URI or a list of queries.
   *
   * @generated from oneof google.cloud.dataproc.v1.HiveJob.queries
   */
  queries: {
    /**
     * The HCFS URI of the script that contains Hive queries.
     *
     * @generated from field: string query_file_uri = 1;
     */
    value: string;
    case: "queryFileUri";
  } | {
    /**
     * A list of queries.
     *
     * @generated from field: google.cloud.dataproc.v1.QueryList query_list = 2;
     */
    value: QueryList;
    case: "queryList";
  } | { case: undefined; value?: undefined } = { case: undefined };

  /**
   * [Optional] Whether to continue executing queries if a query fails.
   * The default value is `false`. Setting to `true` can be useful when executing
   * independent parallel queries.
   *
   * @generated from field: bool continue_on_failure = 3;
   */
  continueOnFailure = false;

  /**
   * [Optional] Mapping of query variable names to values (equivalent to the
   * Hive command: `SET name="value";`).
   *
   * @generated from field: map<string, string> script_variables = 4;
   */
  scriptVariables: { [key: string]: string } = {};

  /**
   * [Optional] A mapping of property names and values, used to configure Hive.
   * Properties that conflict with values set by the Cloud Dataproc API may be
   * overwritten. Can include properties set in /etc/hadoop/conf/*-site.xml,
   * /etc/hive/conf/hive-site.xml, and classes in user code.
   *
   * @generated from field: map<string, string> properties = 5;
   */
  properties: { [key: string]: string } = {};

  /**
   * [Optional] HCFS URIs of jar files to add to the CLASSPATH of the
   * Hive server and Hadoop MapReduce (MR) tasks. Can contain Hive SerDes
   * and UDFs.
   *
   * @generated from field: repeated string jar_file_uris = 6;
   */
  jarFileUris: string[] = [];

  constructor(data?: PartialMessage<HiveJob>) {
    super();
    proto3.util.initPartial(data, this);
  }

  static readonly runtime: typeof proto3 = proto3;
  static readonly typeName = "google.cloud.dataproc.v1.HiveJob";
  static readonly fields: FieldList = proto3.util.newFieldList(() => [
    { no: 1, name: "query_file_uri", kind: "scalar", T: 9 /* ScalarType.STRING */, oneof: "queries" },
    { no: 2, name: "query_list", kind: "message", T: QueryList, oneof: "queries" },
    { no: 3, name: "continue_on_failure", kind: "scalar", T: 8 /* ScalarType.BOOL */ },
    { no: 4, name: "script_variables", kind: "map", K: 9 /* ScalarType.STRING */, V: {kind: "scalar", T: 9 /* ScalarType.STRING */} },
    { no: 5, name: "properties", kind: "map", K: 9 /* ScalarType.STRING */, V: {kind: "scalar", T: 9 /* ScalarType.STRING */} },
    { no: 6, name: "jar_file_uris", kind: "scalar", T: 9 /* ScalarType.STRING */, repeated: true },
  ]);

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): HiveJob {
    return new HiveJob().fromBinary(bytes, options);
  }

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): HiveJob {
    return new HiveJob().fromJson(jsonValue, options);
  }

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): HiveJob {
    return new HiveJob().fromJsonString(jsonString, options);
  }

  static equals(a: HiveJob | PlainMessage<HiveJob> | undefined, b: HiveJob | PlainMessage<HiveJob> | undefined): boolean {
    return proto3.util.equals(HiveJob, a, b);
  }
}

/**
 * A Cloud Dataproc job for running [Apache Spark SQL](http://spark.apache.org/sql/)
 * queries.
 *
 * @generated from message google.cloud.dataproc.v1.SparkSqlJob
 */
export class SparkSqlJob extends Message<SparkSqlJob> {
  /**
   * [Required] The sequence of Spark SQL queries to execute, specified as
   * either an HCFS file URI or as a list of queries.
   *
   * @generated from oneof google.cloud.dataproc.v1.SparkSqlJob.queries
   */
  queries: {
    /**
     * The HCFS URI of the script that contains SQL queries.
     *
     * @generated from field: string query_file_uri = 1;
     */
    value: string;
    case: "queryFileUri";
  } | {
    /**
     * A list of queries.
     *
     * @generated from field: google.cloud.dataproc.v1.QueryList query_list = 2;
     */
    value: QueryList;
    case: "queryList";
  } | { case: undefined; value?: undefined } = { case: undefined };

  /**
   * [Optional] Mapping of query variable names to values (equivalent to the
   * Spark SQL command: SET `name="value";`).
   *
   * @generated from field: map<string, string> script_variables = 3;
   */
  scriptVariables: { [key: string]: string } = {};

  /**
   * [Optional] A mapping of property names to values, used to configure
   * Spark SQL's SparkConf. Properties that conflict with values set by the
   * Cloud Dataproc API may be overwritten.
   *
   * @generated from field: map<string, string> properties = 4;
   */
  properties: { [key: string]: string } = {};

  /**
   * [Optional] HCFS URIs of jar files to be added to the Spark CLASSPATH.
   *
   * @generated from field: repeated string jar_file_uris = 56;
   */
  jarFileUris: string[] = [];

  /**
   * [Optional] The runtime log config for job execution.
   *
   * @generated from field: google.cloud.dataproc.v1.LoggingConfig logging_config = 6;
   */
  loggingConfig?: LoggingConfig;

  constructor(data?: PartialMessage<SparkSqlJob>) {
    super();
    proto3.util.initPartial(data, this);
  }

  static readonly runtime: typeof proto3 = proto3;
  static readonly typeName = "google.cloud.dataproc.v1.SparkSqlJob";
  static readonly fields: FieldList = proto3.util.newFieldList(() => [
    { no: 1, name: "query_file_uri", kind: "scalar", T: 9 /* ScalarType.STRING */, oneof: "queries" },
    { no: 2, name: "query_list", kind: "message", T: QueryList, oneof: "queries" },
    { no: 3, name: "script_variables", kind: "map", K: 9 /* ScalarType.STRING */, V: {kind: "scalar", T: 9 /* ScalarType.STRING */} },
    { no: 4, name: "properties", kind: "map", K: 9 /* ScalarType.STRING */, V: {kind: "scalar", T: 9 /* ScalarType.STRING */} },
    { no: 56, name: "jar_file_uris", kind: "scalar", T: 9 /* ScalarType.STRING */, repeated: true },
    { no: 6, name: "logging_config", kind: "message", T: LoggingConfig },
  ]);

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): SparkSqlJob {
    return new SparkSqlJob().fromBinary(bytes, options);
  }

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): SparkSqlJob {
    return new SparkSqlJob().fromJson(jsonValue, options);
  }

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): SparkSqlJob {
    return new SparkSqlJob().fromJsonString(jsonString, options);
  }

  static equals(a: SparkSqlJob | PlainMessage<SparkSqlJob> | undefined, b: SparkSqlJob | PlainMessage<SparkSqlJob> | undefined): boolean {
    return proto3.util.equals(SparkSqlJob, a, b);
  }
}

/**
 * A Cloud Dataproc job for running [Apache Pig](https://pig.apache.org/)
 * queries on YARN.
 *
 * @generated from message google.cloud.dataproc.v1.PigJob
 */
export class PigJob extends Message<PigJob> {
  /**
   * [Required] The sequence of Pig queries to execute, specified as an HCFS
   * file URI or a list of queries.
   *
   * @generated from oneof google.cloud.dataproc.v1.PigJob.queries
   */
  queries: {
    /**
     * The HCFS URI of the script that contains the Pig queries.
     *
     * @generated from field: string query_file_uri = 1;
     */
    value: string;
    case: "queryFileUri";
  } | {
    /**
     * A list of queries.
     *
     * @generated from field: google.cloud.dataproc.v1.QueryList query_list = 2;
     */
    value: QueryList;
    case: "queryList";
  } | { case: undefined; value?: undefined } = { case: undefined };

  /**
   * [Optional] Whether to continue executing queries if a query fails.
   * The default value is `false`. Setting to `true` can be useful when executing
   * independent parallel queries.
   *
   * @generated from field: bool continue_on_failure = 3;
   */
  continueOnFailure = false;

  /**
   * [Optional] Mapping of query variable names to values (equivalent to the Pig
   * command: `name=[value]`).
   *
   * @generated from field: map<string, string> script_variables = 4;
   */
  scriptVariables: { [key: string]: string } = {};

  /**
   * [Optional] A mapping of property names to values, used to configure Pig.
   * Properties that conflict with values set by the Cloud Dataproc API may be
   * overwritten. Can include properties set in /etc/hadoop/conf/*-site.xml,
   * /etc/pig/conf/pig.properties, and classes in user code.
   *
   * @generated from field: map<string, string> properties = 5;
   */
  properties: { [key: string]: string } = {};

  /**
   * [Optional] HCFS URIs of jar files to add to the CLASSPATH of
   * the Pig Client and Hadoop MapReduce (MR) tasks. Can contain Pig UDFs.
   *
   * @generated from field: repeated string jar_file_uris = 6;
   */
  jarFileUris: string[] = [];

  /**
   * [Optional] The runtime log config for job execution.
   *
   * @generated from field: google.cloud.dataproc.v1.LoggingConfig logging_config = 7;
   */
  loggingConfig?: LoggingConfig;

  constructor(data?: PartialMessage<PigJob>) {
    super();
    proto3.util.initPartial(data, this);
  }

  static readonly runtime: typeof proto3 = proto3;
  static readonly typeName = "google.cloud.dataproc.v1.PigJob";
  static readonly fields: FieldList = proto3.util.newFieldList(() => [
    { no: 1, name: "query_file_uri", kind: "scalar", T: 9 /* ScalarType.STRING */, oneof: "queries" },
    { no: 2, name: "query_list", kind: "message", T: QueryList, oneof: "queries" },
    { no: 3, name: "continue_on_failure", kind: "scalar", T: 8 /* ScalarType.BOOL */ },
    { no: 4, name: "script_variables", kind: "map", K: 9 /* ScalarType.STRING */, V: {kind: "scalar", T: 9 /* ScalarType.STRING */} },
    { no: 5, name: "properties", kind: "map", K: 9 /* ScalarType.STRING */, V: {kind: "scalar", T: 9 /* ScalarType.STRING */} },
    { no: 6, name: "jar_file_uris", kind: "scalar", T: 9 /* ScalarType.STRING */, repeated: true },
    { no: 7, name: "logging_config", kind: "message", T: LoggingConfig },
  ]);

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): PigJob {
    return new PigJob().fromBinary(bytes, options);
  }

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): PigJob {
    return new PigJob().fromJson(jsonValue, options);
  }

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): PigJob {
    return new PigJob().fromJsonString(jsonString, options);
  }

  static equals(a: PigJob | PlainMessage<PigJob> | undefined, b: PigJob | PlainMessage<PigJob> | undefined): boolean {
    return proto3.util.equals(PigJob, a, b);
  }
}

/**
 * Cloud Dataproc job config.
 *
 * @generated from message google.cloud.dataproc.v1.JobPlacement
 */
export class JobPlacement extends Message<JobPlacement> {
  /**
   * [Required] The name of the cluster where the job will be submitted.
   *
   * @generated from field: string cluster_name = 1;
   */
  clusterName = "";

  /**
   * [Output-only] A cluster UUID generated by the Cloud Dataproc service when
   * the job is submitted.
   *
   * @generated from field: string cluster_uuid = 2;
   */
  clusterUuid = "";

  constructor(data?: PartialMessage<JobPlacement>) {
    super();
    proto3.util.initPartial(data, this);
  }

  static readonly runtime: typeof proto3 = proto3;
  static readonly typeName = "google.cloud.dataproc.v1.JobPlacement";
  static readonly fields: FieldList = proto3.util.newFieldList(() => [
    { no: 1, name: "cluster_name", kind: "scalar", T: 9 /* ScalarType.STRING */ },
    { no: 2, name: "cluster_uuid", kind: "scalar", T: 9 /* ScalarType.STRING */ },
  ]);

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): JobPlacement {
    return new JobPlacement().fromBinary(bytes, options);
  }

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): JobPlacement {
    return new JobPlacement().fromJson(jsonValue, options);
  }

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): JobPlacement {
    return new JobPlacement().fromJsonString(jsonString, options);
  }

  static equals(a: JobPlacement | PlainMessage<JobPlacement> | undefined, b: JobPlacement | PlainMessage<JobPlacement> | undefined): boolean {
    return proto3.util.equals(JobPlacement, a, b);
  }
}

/**
 * Cloud Dataproc job status.
 *
 * @generated from message google.cloud.dataproc.v1.JobStatus
 */
export class JobStatus extends Message<JobStatus> {
  /**
   * [Output-only] A state message specifying the overall job state.
   *
   * @generated from field: google.cloud.dataproc.v1.JobStatus.State state = 1;
   */
  state = JobStatus_State.STATE_UNSPECIFIED;

  /**
   * [Output-only] Optional job state details, such as an error
   * description if the state is <code>ERROR</code>.
   *
   * @generated from field: string details = 2;
   */
  details = "";

  /**
   * [Output-only] The time when this state was entered.
   *
   * @generated from field: google.protobuf.Timestamp state_start_time = 6;
   */
  stateStartTime?: Timestamp;

  constructor(data?: PartialMessage<JobStatus>) {
    super();
    proto3.util.initPartial(data, this);
  }

  static readonly runtime: typeof proto3 = proto3;
  static readonly typeName = "google.cloud.dataproc.v1.JobStatus";
  static readonly fields: FieldList = proto3.util.newFieldList(() => [
    { no: 1, name: "state", kind: "enum", T: proto3.getEnumType(JobStatus_State) },
    { no: 2, name: "details", kind: "scalar", T: 9 /* ScalarType.STRING */ },
    { no: 6, name: "state_start_time", kind: "message", T: Timestamp },
  ]);

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): JobStatus {
    return new JobStatus().fromBinary(bytes, options);
  }

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): JobStatus {
    return new JobStatus().fromJson(jsonValue, options);
  }

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): JobStatus {
    return new JobStatus().fromJsonString(jsonString, options);
  }

  static equals(a: JobStatus | PlainMessage<JobStatus> | undefined, b: JobStatus | PlainMessage<JobStatus> | undefined): boolean {
    return proto3.util.equals(JobStatus, a, b);
  }
}

/**
 * The job state.
 *
 * @generated from enum google.cloud.dataproc.v1.JobStatus.State
 */
export enum JobStatus_State {
  /**
   * The job state is unknown.
   *
   * @generated from enum value: STATE_UNSPECIFIED = 0;
   */
  STATE_UNSPECIFIED = 0,

  /**
   * The job is pending; it has been submitted, but is not yet running.
   *
   * @generated from enum value: PENDING = 1;
   */
  PENDING = 1,

  /**
   * Job has been received by the service and completed initial setup;
   * it will soon be submitted to the cluster.
   *
   * @generated from enum value: SETUP_DONE = 8;
   */
  SETUP_DONE = 8,

  /**
   * The job is running on the cluster.
   *
   * @generated from enum value: RUNNING = 2;
   */
  RUNNING = 2,

  /**
   * A CancelJob request has been received, but is pending.
   *
   * @generated from enum value: CANCEL_PENDING = 3;
   */
  CANCEL_PENDING = 3,

  /**
   * Transient in-flight resources have been canceled, and the request to
   * cancel the running job has been issued to the cluster.
   *
   * @generated from enum value: CANCEL_STARTED = 7;
   */
  CANCEL_STARTED = 7,

  /**
   * The job cancellation was successful.
   *
   * @generated from enum value: CANCELLED = 4;
   */
  CANCELLED = 4,

  /**
   * The job has completed successfully.
   *
   * @generated from enum value: DONE = 5;
   */
  DONE = 5,

  /**
   * The job has completed, but encountered an error.
   *
   * @generated from enum value: ERROR = 6;
   */
  ERROR = 6,
}
// Retrieve enum metadata with: proto3.getEnumType(JobStatus_State)
proto3.util.setEnumType(JobStatus_State, "google.cloud.dataproc.v1.JobStatus.State", [
  { no: 0, name: "STATE_UNSPECIFIED" },
  { no: 1, name: "PENDING" },
  { no: 8, name: "SETUP_DONE" },
  { no: 2, name: "RUNNING" },
  { no: 3, name: "CANCEL_PENDING" },
  { no: 7, name: "CANCEL_STARTED" },
  { no: 4, name: "CANCELLED" },
  { no: 5, name: "DONE" },
  { no: 6, name: "ERROR" },
]);

/**
 * Encapsulates the full scoping used to reference a job.
 *
 * @generated from message google.cloud.dataproc.v1.JobReference
 */
export class JobReference extends Message<JobReference> {
  /**
   * [Required] The ID of the Google Cloud Platform project that the job
   * belongs to.
   *
   * @generated from field: string project_id = 1;
   */
  projectId = "";

  /**
   * [Optional] The job ID, which must be unique within the project. The job ID
   * is generated by the server upon job submission or provided by the user as a
   * means to perform retries without creating duplicate jobs. The ID must
   * contain only letters (a-z, A-Z), numbers (0-9), underscores (_), or
   * hyphens (-). The maximum length is 512 characters.
   *
   * @generated from field: string job_id = 2;
   */
  jobId = "";

  constructor(data?: PartialMessage<JobReference>) {
    super();
    proto3.util.initPartial(data, this);
  }

  static readonly runtime: typeof proto3 = proto3;
  static readonly typeName = "google.cloud.dataproc.v1.JobReference";
  static readonly fields: FieldList = proto3.util.newFieldList(() => [
    { no: 1, name: "project_id", kind: "scalar", T: 9 /* ScalarType.STRING */ },
    { no: 2, name: "job_id", kind: "scalar", T: 9 /* ScalarType.STRING */ },
  ]);

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): JobReference {
    return new JobReference().fromBinary(bytes, options);
  }

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): JobReference {
    return new JobReference().fromJson(jsonValue, options);
  }

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): JobReference {
    return new JobReference().fromJsonString(jsonString, options);
  }

  static equals(a: JobReference | PlainMessage<JobReference> | undefined, b: JobReference | PlainMessage<JobReference> | undefined): boolean {
    return proto3.util.equals(JobReference, a, b);
  }
}

/**
 * A Cloud Dataproc job resource.
 *
 * @generated from message google.cloud.dataproc.v1.Job
 */
export class Job extends Message<Job> {
  /**
   * [Optional] The fully qualified reference to the job, which can be used to
   * obtain the equivalent REST path of the job resource. If this property
   * is not specified when a job is created, the server generates a
   * <code>job_id</code>.
   *
   * @generated from field: google.cloud.dataproc.v1.JobReference reference = 1;
   */
  reference?: JobReference;

  /**
   * [Required] Job information, including how, when, and where to
   * run the job.
   *
   * @generated from field: google.cloud.dataproc.v1.JobPlacement placement = 2;
   */
  placement?: JobPlacement;

  /**
   * [Required] The application/framework-specific portion of the job.
   *
   * @generated from oneof google.cloud.dataproc.v1.Job.type_job
   */
  typeJob: {
    /**
     * Job is a Hadoop job.
     *
     * @generated from field: google.cloud.dataproc.v1.HadoopJob hadoop_job = 3;
     */
    value: HadoopJob;
    case: "hadoopJob";
  } | {
    /**
     * Job is a Spark job.
     *
     * @generated from field: google.cloud.dataproc.v1.SparkJob spark_job = 4;
     */
    value: SparkJob;
    case: "sparkJob";
  } | {
    /**
     * Job is a Pyspark job.
     *
     * @generated from field: google.cloud.dataproc.v1.PySparkJob pyspark_job = 5;
     */
    value: PySparkJob;
    case: "pysparkJob";
  } | {
    /**
     * Job is a Hive job.
     *
     * @generated from field: google.cloud.dataproc.v1.HiveJob hive_job = 6;
     */
    value: HiveJob;
    case: "hiveJob";
  } | {
    /**
     * Job is a Pig job.
     *
     * @generated from field: google.cloud.dataproc.v1.PigJob pig_job = 7;
     */
    value: PigJob;
    case: "pigJob";
  } | {
    /**
     * Job is a SparkSql job.
     *
     * @generated from field: google.cloud.dataproc.v1.SparkSqlJob spark_sql_job = 12;
     */
    value: SparkSqlJob;
    case: "sparkSqlJob";
  } | { case: undefined; value?: undefined } = { case: undefined };

  /**
   * [Output-only] The job status. Additional application-specific
   * status information may be contained in the <code>type_job</code>
   * and <code>yarn_applications</code> fields.
   *
   * @generated from field: google.cloud.dataproc.v1.JobStatus status = 8;
   */
  status?: JobStatus;

  /**
   * [Output-only] The previous job status.
   *
   * @generated from field: repeated google.cloud.dataproc.v1.JobStatus status_history = 13;
   */
  statusHistory: JobStatus[] = [];

  /**
   * [Output-only] A URI pointing to the location of the stdout of the job's
   * driver program.
   *
   * @generated from field: string driver_output_resource_uri = 17;
   */
  driverOutputResourceUri = "";

  /**
   * [Output-only] If present, the location of miscellaneous control files
   * which may be used as part of job setup and handling. If not present,
   * control files may be placed in the same location as `driver_output_uri`.
   *
   * @generated from field: string driver_control_files_uri = 15;
   */
  driverControlFilesUri = "";

  constructor(data?: PartialMessage<Job>) {
    super();
    proto3.util.initPartial(data, this);
  }

  static readonly runtime: typeof proto3 = proto3;
  static readonly typeName = "google.cloud.dataproc.v1.Job";
  static readonly fields: FieldList = proto3.util.newFieldList(() => [
    { no: 1, name: "reference", kind: "message", T: JobReference },
    { no: 2, name: "placement", kind: "message", T: JobPlacement },
    { no: 3, name: "hadoop_job", kind: "message", T: HadoopJob, oneof: "type_job" },
    { no: 4, name: "spark_job", kind: "message", T: SparkJob, oneof: "type_job" },
    { no: 5, name: "pyspark_job", kind: "message", T: PySparkJob, oneof: "type_job" },
    { no: 6, name: "hive_job", kind: "message", T: HiveJob, oneof: "type_job" },
    { no: 7, name: "pig_job", kind: "message", T: PigJob, oneof: "type_job" },
    { no: 12, name: "spark_sql_job", kind: "message", T: SparkSqlJob, oneof: "type_job" },
    { no: 8, name: "status", kind: "message", T: JobStatus },
    { no: 13, name: "status_history", kind: "message", T: JobStatus, repeated: true },
    { no: 17, name: "driver_output_resource_uri", kind: "scalar", T: 9 /* ScalarType.STRING */ },
    { no: 15, name: "driver_control_files_uri", kind: "scalar", T: 9 /* ScalarType.STRING */ },
  ]);

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): Job {
    return new Job().fromBinary(bytes, options);
  }

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): Job {
    return new Job().fromJson(jsonValue, options);
  }

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): Job {
    return new Job().fromJsonString(jsonString, options);
  }

  static equals(a: Job | PlainMessage<Job> | undefined, b: Job | PlainMessage<Job> | undefined): boolean {
    return proto3.util.equals(Job, a, b);
  }
}

/**
 * A request to submit a job.
 *
 * @generated from message google.cloud.dataproc.v1.SubmitJobRequest
 */
export class SubmitJobRequest extends Message<SubmitJobRequest> {
  /**
   * [Required] The ID of the Google Cloud Platform project that the job
   * belongs to.
   *
   * @generated from field: string project_id = 1;
   */
  projectId = "";

  /**
   * [Required] The Cloud Dataproc region in which to handle the request.
   *
   * @generated from field: string region = 3;
   */
  region = "";

  /**
   * [Required] The job resource.
   *
   * @generated from field: google.cloud.dataproc.v1.Job job = 2;
   */
  job?: Job;

  constructor(data?: PartialMessage<SubmitJobRequest>) {
    super();
    proto3.util.initPartial(data, this);
  }

  static readonly runtime: typeof proto3 = proto3;
  static readonly typeName = "google.cloud.dataproc.v1.SubmitJobRequest";
  static readonly fields: FieldList = proto3.util.newFieldList(() => [
    { no: 1, name: "project_id", kind: "scalar", T: 9 /* ScalarType.STRING */ },
    { no: 3, name: "region", kind: "scalar", T: 9 /* ScalarType.STRING */ },
    { no: 2, name: "job", kind: "message", T: Job },
  ]);

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): SubmitJobRequest {
    return new SubmitJobRequest().fromBinary(bytes, options);
  }

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): SubmitJobRequest {
    return new SubmitJobRequest().fromJson(jsonValue, options);
  }

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): SubmitJobRequest {
    return new SubmitJobRequest().fromJsonString(jsonString, options);
  }

  static equals(a: SubmitJobRequest | PlainMessage<SubmitJobRequest> | undefined, b: SubmitJobRequest | PlainMessage<SubmitJobRequest> | undefined): boolean {
    return proto3.util.equals(SubmitJobRequest, a, b);
  }
}

/**
 * A request to get the resource representation for a job in a project.
 *
 * @generated from message google.cloud.dataproc.v1.GetJobRequest
 */
export class GetJobRequest extends Message<GetJobRequest> {
  /**
   * [Required] The ID of the Google Cloud Platform project that the job
   * belongs to.
   *
   * @generated from field: string project_id = 1;
   */
  projectId = "";

  /**
   * [Required] The Cloud Dataproc region in which to handle the request.
   *
   * @generated from field: string region = 3;
   */
  region = "";

  /**
   * [Required] The job ID.
   *
   * @generated from field: string job_id = 2;
   */
  jobId = "";

  constructor(data?: PartialMessage<GetJobRequest>) {
    super();
    proto3.util.initPartial(data, this);
  }

  static readonly runtime: typeof proto3 = proto3;
  static readonly typeName = "google.cloud.dataproc.v1.GetJobRequest";
  static readonly fields: FieldList = proto3.util.newFieldList(() => [
    { no: 1, name: "project_id", kind: "scalar", T: 9 /* ScalarType.STRING */ },
    { no: 3, name: "region", kind: "scalar", T: 9 /* ScalarType.STRING */ },
    { no: 2, name: "job_id", kind: "scalar", T: 9 /* ScalarType.STRING */ },
  ]);

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): GetJobRequest {
    return new GetJobRequest().fromBinary(bytes, options);
  }

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): GetJobRequest {
    return new GetJobRequest().fromJson(jsonValue, options);
  }

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): GetJobRequest {
    return new GetJobRequest().fromJsonString(jsonString, options);
  }

  static equals(a: GetJobRequest | PlainMessage<GetJobRequest> | undefined, b: GetJobRequest | PlainMessage<GetJobRequest> | undefined): boolean {
    return proto3.util.equals(GetJobRequest, a, b);
  }
}

/**
 * A request to list jobs in a project.
 *
 * @generated from message google.cloud.dataproc.v1.ListJobsRequest
 */
export class ListJobsRequest extends Message<ListJobsRequest> {
  /**
   * [Required] The ID of the Google Cloud Platform project that the job
   * belongs to.
   *
   * @generated from field: string project_id = 1;
   */
  projectId = "";

  /**
   * [Required] The Cloud Dataproc region in which to handle the request.
   *
   * @generated from field: string region = 6;
   */
  region = "";

  /**
   * [Optional] The number of results to return in each response.
   *
   * @generated from field: int32 page_size = 2;
   */
  pageSize = 0;

  /**
   * [Optional] The page token, returned by a previous call, to request the
   * next page of results.
   *
   * @generated from field: string page_token = 3;
   */
  pageToken = "";

  /**
   * [Optional] If set, the returned jobs list includes only jobs that were
   * submitted to the named cluster.
   *
   * @generated from field: string cluster_name = 4;
   */
  clusterName = "";

  /**
   * [Optional] Specifies enumerated categories of jobs to list
   * (default = match ALL jobs).
   *
   * @generated from field: google.cloud.dataproc.v1.ListJobsRequest.JobStateMatcher job_state_matcher = 5;
   */
  jobStateMatcher = ListJobsRequest_JobStateMatcher.ALL;

  constructor(data?: PartialMessage<ListJobsRequest>) {
    super();
    proto3.util.initPartial(data, this);
  }

  static readonly runtime: typeof proto3 = proto3;
  static readonly typeName = "google.cloud.dataproc.v1.ListJobsRequest";
  static readonly fields: FieldList = proto3.util.newFieldList(() => [
    { no: 1, name: "project_id", kind: "scalar", T: 9 /* ScalarType.STRING */ },
    { no: 6, name: "region", kind: "scalar", T: 9 /* ScalarType.STRING */ },
    { no: 2, name: "page_size", kind: "scalar", T: 5 /* ScalarType.INT32 */ },
    { no: 3, name: "page_token", kind: "scalar", T: 9 /* ScalarType.STRING */ },
    { no: 4, name: "cluster_name", kind: "scalar", T: 9 /* ScalarType.STRING */ },
    { no: 5, name: "job_state_matcher", kind: "enum", T: proto3.getEnumType(ListJobsRequest_JobStateMatcher) },
  ]);

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): ListJobsRequest {
    return new ListJobsRequest().fromBinary(bytes, options);
  }

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): ListJobsRequest {
    return new ListJobsRequest().fromJson(jsonValue, options);
  }

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): ListJobsRequest {
    return new ListJobsRequest().fromJsonString(jsonString, options);
  }

  static equals(a: ListJobsRequest | PlainMessage<ListJobsRequest> | undefined, b: ListJobsRequest | PlainMessage<ListJobsRequest> | undefined): boolean {
    return proto3.util.equals(ListJobsRequest, a, b);
  }
}

/**
 * A matcher that specifies categories of job states.
 *
 * @generated from enum google.cloud.dataproc.v1.ListJobsRequest.JobStateMatcher
 */
export enum ListJobsRequest_JobStateMatcher {
  /**
   * Match all jobs, regardless of state.
   *
   * @generated from enum value: ALL = 0;
   */
  ALL = 0,

  /**
   * Only match jobs in non-terminal states: PENDING, RUNNING, or
   * CANCEL_PENDING.
   *
   * @generated from enum value: ACTIVE = 1;
   */
  ACTIVE = 1,

  /**
   * Only match jobs in terminal states: CANCELLED, DONE, or ERROR.
   *
   * @generated from enum value: NON_ACTIVE = 2;
   */
  NON_ACTIVE = 2,
}
// Retrieve enum metadata with: proto3.getEnumType(ListJobsRequest_JobStateMatcher)
proto3.util.setEnumType(ListJobsRequest_JobStateMatcher, "google.cloud.dataproc.v1.ListJobsRequest.JobStateMatcher", [
  { no: 0, name: "ALL" },
  { no: 1, name: "ACTIVE" },
  { no: 2, name: "NON_ACTIVE" },
]);

/**
 * A list of jobs in a project.
 *
 * @generated from message google.cloud.dataproc.v1.ListJobsResponse
 */
export class ListJobsResponse extends Message<ListJobsResponse> {
  /**
   * [Output-only] Jobs list.
   *
   * @generated from field: repeated google.cloud.dataproc.v1.Job jobs = 1;
   */
  jobs: Job[] = [];

  /**
   * [Optional] This token is included in the response if there are more results
   * to fetch. To fetch additional results, provide this value as the
   * `page_token` in a subsequent <code>ListJobsRequest</code>.
   *
   * @generated from field: string next_page_token = 2;
   */
  nextPageToken = "";

  constructor(data?: PartialMessage<ListJobsResponse>) {
    super();
    proto3.util.initPartial(data, this);
  }

  static readonly runtime: typeof proto3 = proto3;
  static readonly typeName = "google.cloud.dataproc.v1.ListJobsResponse";
  static readonly fields: FieldList = proto3.util.newFieldList(() => [
    { no: 1, name: "jobs", kind: "message", T: Job, repeated: true },
    { no: 2, name: "next_page_token", kind: "scalar", T: 9 /* ScalarType.STRING */ },
  ]);

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): ListJobsResponse {
    return new ListJobsResponse().fromBinary(bytes, options);
  }

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): ListJobsResponse {
    return new ListJobsResponse().fromJson(jsonValue, options);
  }

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): ListJobsResponse {
    return new ListJobsResponse().fromJsonString(jsonString, options);
  }

  static equals(a: ListJobsResponse | PlainMessage<ListJobsResponse> | undefined, b: ListJobsResponse | PlainMessage<ListJobsResponse> | undefined): boolean {
    return proto3.util.equals(ListJobsResponse, a, b);
  }
}

/**
 * A request to cancel a job.
 *
 * @generated from message google.cloud.dataproc.v1.CancelJobRequest
 */
export class CancelJobRequest extends Message<CancelJobRequest> {
  /**
   * [Required] The ID of the Google Cloud Platform project that the job
   * belongs to.
   *
   * @generated from field: string project_id = 1;
   */
  projectId = "";

  /**
   * [Required] The Cloud Dataproc region in which to handle the request.
   *
   * @generated from field: string region = 3;
   */
  region = "";

  /**
   * [Required] The job ID.
   *
   * @generated from field: string job_id = 2;
   */
  jobId = "";

  constructor(data?: PartialMessage<CancelJobRequest>) {
    super();
    proto3.util.initPartial(data, this);
  }

  static readonly runtime: typeof proto3 = proto3;
  static readonly typeName = "google.cloud.dataproc.v1.CancelJobRequest";
  static readonly fields: FieldList = proto3.util.newFieldList(() => [
    { no: 1, name: "project_id", kind: "scalar", T: 9 /* ScalarType.STRING */ },
    { no: 3, name: "region", kind: "scalar", T: 9 /* ScalarType.STRING */ },
    { no: 2, name: "job_id", kind: "scalar", T: 9 /* ScalarType.STRING */ },
  ]);

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): CancelJobRequest {
    return new CancelJobRequest().fromBinary(bytes, options);
  }

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): CancelJobRequest {
    return new CancelJobRequest().fromJson(jsonValue, options);
  }

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): CancelJobRequest {
    return new CancelJobRequest().fromJsonString(jsonString, options);
  }

  static equals(a: CancelJobRequest | PlainMessage<CancelJobRequest> | undefined, b: CancelJobRequest | PlainMessage<CancelJobRequest> | undefined): boolean {
    return proto3.util.equals(CancelJobRequest, a, b);
  }
}

/**
 * A request to delete a job.
 *
 * @generated from message google.cloud.dataproc.v1.DeleteJobRequest
 */
export class DeleteJobRequest extends Message<DeleteJobRequest> {
  /**
   * [Required] The ID of the Google Cloud Platform project that the job
   * belongs to.
   *
   * @generated from field: string project_id = 1;
   */
  projectId = "";

  /**
   * [Required] The Cloud Dataproc region in which to handle the request.
   *
   * @generated from field: string region = 3;
   */
  region = "";

  /**
   * [Required] The job ID.
   *
   * @generated from field: string job_id = 2;
   */
  jobId = "";

  constructor(data?: PartialMessage<DeleteJobRequest>) {
    super();
    proto3.util.initPartial(data, this);
  }

  static readonly runtime: typeof proto3 = proto3;
  static readonly typeName = "google.cloud.dataproc.v1.DeleteJobRequest";
  static readonly fields: FieldList = proto3.util.newFieldList(() => [
    { no: 1, name: "project_id", kind: "scalar", T: 9 /* ScalarType.STRING */ },
    { no: 3, name: "region", kind: "scalar", T: 9 /* ScalarType.STRING */ },
    { no: 2, name: "job_id", kind: "scalar", T: 9 /* ScalarType.STRING */ },
  ]);

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): DeleteJobRequest {
    return new DeleteJobRequest().fromBinary(bytes, options);
  }

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): DeleteJobRequest {
    return new DeleteJobRequest().fromJson(jsonValue, options);
  }

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): DeleteJobRequest {
    return new DeleteJobRequest().fromJsonString(jsonString, options);
  }

  static equals(a: DeleteJobRequest | PlainMessage<DeleteJobRequest> | undefined, b: DeleteJobRequest | PlainMessage<DeleteJobRequest> | undefined): boolean {
    return proto3.util.equals(DeleteJobRequest, a, b);
  }
}


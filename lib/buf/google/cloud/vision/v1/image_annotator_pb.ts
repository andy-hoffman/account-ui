// Copyright 2016 Google Inc.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

// @generated by protoc-gen-es v1.3.0 with parameter "target=ts"
// @generated from file google/cloud/vision/v1/image_annotator.proto (package google.cloud.vision.v1, syntax proto3)
/* eslint-disable */
// @ts-nocheck

import type { BinaryReadOptions, FieldList, JsonReadOptions, JsonValue, PartialMessage, PlainMessage } from "@bufbuild/protobuf";
import { Message, proto3 } from "@bufbuild/protobuf";
import { BoundingPoly, Position } from "./geometry_pb.js";
import { LatLng } from "../../../type/latlng_pb.js";
import { Color } from "../../../type/color_pb.js";
import { TextAnnotation } from "./text_annotation_pb.js";
import { WebDetection } from "./web_detection_pb.js";
import { Status } from "../../../rpc/status_pb.js";

/**
 * A bucketized representation of likelihood, which is intended to give clients
 * highly stable results across model upgrades.
 *
 * @generated from enum google.cloud.vision.v1.Likelihood
 */
export enum Likelihood {
  /**
   * Unknown likelihood.
   *
   * @generated from enum value: UNKNOWN = 0;
   */
  UNKNOWN = 0,

  /**
   * It is very unlikely that the image belongs to the specified vertical.
   *
   * @generated from enum value: VERY_UNLIKELY = 1;
   */
  VERY_UNLIKELY = 1,

  /**
   * It is unlikely that the image belongs to the specified vertical.
   *
   * @generated from enum value: UNLIKELY = 2;
   */
  UNLIKELY = 2,

  /**
   * It is possible that the image belongs to the specified vertical.
   *
   * @generated from enum value: POSSIBLE = 3;
   */
  POSSIBLE = 3,

  /**
   * It is likely that the image belongs to the specified vertical.
   *
   * @generated from enum value: LIKELY = 4;
   */
  LIKELY = 4,

  /**
   * It is very likely that the image belongs to the specified vertical.
   *
   * @generated from enum value: VERY_LIKELY = 5;
   */
  VERY_LIKELY = 5,
}
// Retrieve enum metadata with: proto3.getEnumType(Likelihood)
proto3.util.setEnumType(Likelihood, "google.cloud.vision.v1.Likelihood", [
  { no: 0, name: "UNKNOWN" },
  { no: 1, name: "VERY_UNLIKELY" },
  { no: 2, name: "UNLIKELY" },
  { no: 3, name: "POSSIBLE" },
  { no: 4, name: "LIKELY" },
  { no: 5, name: "VERY_LIKELY" },
]);

/**
 * Users describe the type of Google Cloud Vision API tasks to perform over
 * images by using *Feature*s. Each Feature indicates a type of image
 * detection task to perform. Features encode the Cloud Vision API
 * vertical to operate on and the number of top-scoring results to return.
 *
 * @generated from message google.cloud.vision.v1.Feature
 */
export class Feature extends Message<Feature> {
  /**
   * The feature type.
   *
   * @generated from field: google.cloud.vision.v1.Feature.Type type = 1;
   */
  type = Feature_Type.TYPE_UNSPECIFIED;

  /**
   * Maximum number of results of this type.
   *
   * @generated from field: int32 max_results = 2;
   */
  maxResults = 0;

  constructor(data?: PartialMessage<Feature>) {
    super();
    proto3.util.initPartial(data, this);
  }

  static readonly runtime: typeof proto3 = proto3;
  static readonly typeName = "google.cloud.vision.v1.Feature";
  static readonly fields: FieldList = proto3.util.newFieldList(() => [
    { no: 1, name: "type", kind: "enum", T: proto3.getEnumType(Feature_Type) },
    { no: 2, name: "max_results", kind: "scalar", T: 5 /* ScalarType.INT32 */ },
  ]);

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): Feature {
    return new Feature().fromBinary(bytes, options);
  }

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): Feature {
    return new Feature().fromJson(jsonValue, options);
  }

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): Feature {
    return new Feature().fromJsonString(jsonString, options);
  }

  static equals(a: Feature | PlainMessage<Feature> | undefined, b: Feature | PlainMessage<Feature> | undefined): boolean {
    return proto3.util.equals(Feature, a, b);
  }
}

/**
 * Type of image feature.
 *
 * @generated from enum google.cloud.vision.v1.Feature.Type
 */
export enum Feature_Type {
  /**
   * Unspecified feature type.
   *
   * @generated from enum value: TYPE_UNSPECIFIED = 0;
   */
  TYPE_UNSPECIFIED = 0,

  /**
   * Run face detection.
   *
   * @generated from enum value: FACE_DETECTION = 1;
   */
  FACE_DETECTION = 1,

  /**
   * Run landmark detection.
   *
   * @generated from enum value: LANDMARK_DETECTION = 2;
   */
  LANDMARK_DETECTION = 2,

  /**
   * Run logo detection.
   *
   * @generated from enum value: LOGO_DETECTION = 3;
   */
  LOGO_DETECTION = 3,

  /**
   * Run label detection.
   *
   * @generated from enum value: LABEL_DETECTION = 4;
   */
  LABEL_DETECTION = 4,

  /**
   * Run OCR.
   *
   * @generated from enum value: TEXT_DETECTION = 5;
   */
  TEXT_DETECTION = 5,

  /**
   * Run dense text document OCR. Takes precedence when both
   * DOCUMENT_TEXT_DETECTION and TEXT_DETECTION are present.
   *
   * @generated from enum value: DOCUMENT_TEXT_DETECTION = 11;
   */
  DOCUMENT_TEXT_DETECTION = 11,

  /**
   * Run computer vision models to compute image safe-search properties.
   *
   * @generated from enum value: SAFE_SEARCH_DETECTION = 6;
   */
  SAFE_SEARCH_DETECTION = 6,

  /**
   * Compute a set of image properties, such as the image's dominant colors.
   *
   * @generated from enum value: IMAGE_PROPERTIES = 7;
   */
  IMAGE_PROPERTIES = 7,

  /**
   * Run crop hints.
   *
   * @generated from enum value: CROP_HINTS = 9;
   */
  CROP_HINTS = 9,

  /**
   * Run web detection.
   *
   * @generated from enum value: WEB_DETECTION = 10;
   */
  WEB_DETECTION = 10,
}
// Retrieve enum metadata with: proto3.getEnumType(Feature_Type)
proto3.util.setEnumType(Feature_Type, "google.cloud.vision.v1.Feature.Type", [
  { no: 0, name: "TYPE_UNSPECIFIED" },
  { no: 1, name: "FACE_DETECTION" },
  { no: 2, name: "LANDMARK_DETECTION" },
  { no: 3, name: "LOGO_DETECTION" },
  { no: 4, name: "LABEL_DETECTION" },
  { no: 5, name: "TEXT_DETECTION" },
  { no: 11, name: "DOCUMENT_TEXT_DETECTION" },
  { no: 6, name: "SAFE_SEARCH_DETECTION" },
  { no: 7, name: "IMAGE_PROPERTIES" },
  { no: 9, name: "CROP_HINTS" },
  { no: 10, name: "WEB_DETECTION" },
]);

/**
 * External image source (Google Cloud Storage image location).
 *
 * @generated from message google.cloud.vision.v1.ImageSource
 */
export class ImageSource extends Message<ImageSource> {
  /**
   * NOTE: For new code `image_uri` below is preferred.
   * Google Cloud Storage image URI, which must be in the following form:
   * `gs://bucket_name/object_name` (for details, see
   * [Google Cloud Storage Request
   * URIs](https://cloud.google.com/storage/docs/reference-uris)).
   * NOTE: Cloud Storage object versioning is not supported.
   *
   * @generated from field: string gcs_image_uri = 1;
   */
  gcsImageUri = "";

  /**
   * Image URI which supports:
   * 1) Google Cloud Storage image URI, which must be in the following form:
   * `gs://bucket_name/object_name` (for details, see
   * [Google Cloud Storage Request
   * URIs](https://cloud.google.com/storage/docs/reference-uris)).
   * NOTE: Cloud Storage object versioning is not supported.
   * 2) Publicly accessible image HTTP/HTTPS URL.
   * This is preferred over the legacy `gcs_image_uri` above. When both
   * `gcs_image_uri` and `image_uri` are specified, `image_uri` takes
   * precedence.
   *
   * @generated from field: string image_uri = 2;
   */
  imageUri = "";

  constructor(data?: PartialMessage<ImageSource>) {
    super();
    proto3.util.initPartial(data, this);
  }

  static readonly runtime: typeof proto3 = proto3;
  static readonly typeName = "google.cloud.vision.v1.ImageSource";
  static readonly fields: FieldList = proto3.util.newFieldList(() => [
    { no: 1, name: "gcs_image_uri", kind: "scalar", T: 9 /* ScalarType.STRING */ },
    { no: 2, name: "image_uri", kind: "scalar", T: 9 /* ScalarType.STRING */ },
  ]);

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): ImageSource {
    return new ImageSource().fromBinary(bytes, options);
  }

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): ImageSource {
    return new ImageSource().fromJson(jsonValue, options);
  }

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): ImageSource {
    return new ImageSource().fromJsonString(jsonString, options);
  }

  static equals(a: ImageSource | PlainMessage<ImageSource> | undefined, b: ImageSource | PlainMessage<ImageSource> | undefined): boolean {
    return proto3.util.equals(ImageSource, a, b);
  }
}

/**
 * Client image to perform Google Cloud Vision API tasks over.
 *
 * @generated from message google.cloud.vision.v1.Image
 */
export class Image extends Message<Image> {
  /**
   * Image content, represented as a stream of bytes.
   * Note: as with all `bytes` fields, protobuffers use a pure binary
   * representation, whereas JSON representations use base64.
   *
   * @generated from field: bytes content = 1;
   */
  content = new Uint8Array(0);

  /**
   * Google Cloud Storage image location. If both `content` and `source`
   * are provided for an image, `content` takes precedence and is
   * used to perform the image annotation request.
   *
   * @generated from field: google.cloud.vision.v1.ImageSource source = 2;
   */
  source?: ImageSource;

  constructor(data?: PartialMessage<Image>) {
    super();
    proto3.util.initPartial(data, this);
  }

  static readonly runtime: typeof proto3 = proto3;
  static readonly typeName = "google.cloud.vision.v1.Image";
  static readonly fields: FieldList = proto3.util.newFieldList(() => [
    { no: 1, name: "content", kind: "scalar", T: 12 /* ScalarType.BYTES */ },
    { no: 2, name: "source", kind: "message", T: ImageSource },
  ]);

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): Image {
    return new Image().fromBinary(bytes, options);
  }

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): Image {
    return new Image().fromJson(jsonValue, options);
  }

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): Image {
    return new Image().fromJsonString(jsonString, options);
  }

  static equals(a: Image | PlainMessage<Image> | undefined, b: Image | PlainMessage<Image> | undefined): boolean {
    return proto3.util.equals(Image, a, b);
  }
}

/**
 * A face annotation object contains the results of face detection.
 *
 * @generated from message google.cloud.vision.v1.FaceAnnotation
 */
export class FaceAnnotation extends Message<FaceAnnotation> {
  /**
   * The bounding polygon around the face. The coordinates of the bounding box
   * are in the original image's scale, as returned in `ImageParams`.
   * The bounding box is computed to "frame" the face in accordance with human
   * expectations. It is based on the landmarker results.
   * Note that one or more x and/or y coordinates may not be generated in the
   * `BoundingPoly` (the polygon will be unbounded) if only a partial face
   * appears in the image to be annotated.
   *
   * @generated from field: google.cloud.vision.v1.BoundingPoly bounding_poly = 1;
   */
  boundingPoly?: BoundingPoly;

  /**
   * The `fd_bounding_poly` bounding polygon is tighter than the
   * `boundingPoly`, and encloses only the skin part of the face. Typically, it
   * is used to eliminate the face from any image analysis that detects the
   * "amount of skin" visible in an image. It is not based on the
   * landmarker results, only on the initial face detection, hence
   * the <code>fd</code> (face detection) prefix.
   *
   * @generated from field: google.cloud.vision.v1.BoundingPoly fd_bounding_poly = 2;
   */
  fdBoundingPoly?: BoundingPoly;

  /**
   * Detected face landmarks.
   *
   * @generated from field: repeated google.cloud.vision.v1.FaceAnnotation.Landmark landmarks = 3;
   */
  landmarks: FaceAnnotation_Landmark[] = [];

  /**
   * Roll angle, which indicates the amount of clockwise/anti-clockwise rotation
   * of the face relative to the image vertical about the axis perpendicular to
   * the face. Range [-180,180].
   *
   * @generated from field: float roll_angle = 4;
   */
  rollAngle = 0;

  /**
   * Yaw angle, which indicates the leftward/rightward angle that the face is
   * pointing relative to the vertical plane perpendicular to the image. Range
   * [-180,180].
   *
   * @generated from field: float pan_angle = 5;
   */
  panAngle = 0;

  /**
   * Pitch angle, which indicates the upwards/downwards angle that the face is
   * pointing relative to the image's horizontal plane. Range [-180,180].
   *
   * @generated from field: float tilt_angle = 6;
   */
  tiltAngle = 0;

  /**
   * Detection confidence. Range [0, 1].
   *
   * @generated from field: float detection_confidence = 7;
   */
  detectionConfidence = 0;

  /**
   * Face landmarking confidence. Range [0, 1].
   *
   * @generated from field: float landmarking_confidence = 8;
   */
  landmarkingConfidence = 0;

  /**
   * Joy likelihood.
   *
   * @generated from field: google.cloud.vision.v1.Likelihood joy_likelihood = 9;
   */
  joyLikelihood = Likelihood.UNKNOWN;

  /**
   * Sorrow likelihood.
   *
   * @generated from field: google.cloud.vision.v1.Likelihood sorrow_likelihood = 10;
   */
  sorrowLikelihood = Likelihood.UNKNOWN;

  /**
   * Anger likelihood.
   *
   * @generated from field: google.cloud.vision.v1.Likelihood anger_likelihood = 11;
   */
  angerLikelihood = Likelihood.UNKNOWN;

  /**
   * Surprise likelihood.
   *
   * @generated from field: google.cloud.vision.v1.Likelihood surprise_likelihood = 12;
   */
  surpriseLikelihood = Likelihood.UNKNOWN;

  /**
   * Under-exposed likelihood.
   *
   * @generated from field: google.cloud.vision.v1.Likelihood under_exposed_likelihood = 13;
   */
  underExposedLikelihood = Likelihood.UNKNOWN;

  /**
   * Blurred likelihood.
   *
   * @generated from field: google.cloud.vision.v1.Likelihood blurred_likelihood = 14;
   */
  blurredLikelihood = Likelihood.UNKNOWN;

  /**
   * Headwear likelihood.
   *
   * @generated from field: google.cloud.vision.v1.Likelihood headwear_likelihood = 15;
   */
  headwearLikelihood = Likelihood.UNKNOWN;

  constructor(data?: PartialMessage<FaceAnnotation>) {
    super();
    proto3.util.initPartial(data, this);
  }

  static readonly runtime: typeof proto3 = proto3;
  static readonly typeName = "google.cloud.vision.v1.FaceAnnotation";
  static readonly fields: FieldList = proto3.util.newFieldList(() => [
    { no: 1, name: "bounding_poly", kind: "message", T: BoundingPoly },
    { no: 2, name: "fd_bounding_poly", kind: "message", T: BoundingPoly },
    { no: 3, name: "landmarks", kind: "message", T: FaceAnnotation_Landmark, repeated: true },
    { no: 4, name: "roll_angle", kind: "scalar", T: 2 /* ScalarType.FLOAT */ },
    { no: 5, name: "pan_angle", kind: "scalar", T: 2 /* ScalarType.FLOAT */ },
    { no: 6, name: "tilt_angle", kind: "scalar", T: 2 /* ScalarType.FLOAT */ },
    { no: 7, name: "detection_confidence", kind: "scalar", T: 2 /* ScalarType.FLOAT */ },
    { no: 8, name: "landmarking_confidence", kind: "scalar", T: 2 /* ScalarType.FLOAT */ },
    { no: 9, name: "joy_likelihood", kind: "enum", T: proto3.getEnumType(Likelihood) },
    { no: 10, name: "sorrow_likelihood", kind: "enum", T: proto3.getEnumType(Likelihood) },
    { no: 11, name: "anger_likelihood", kind: "enum", T: proto3.getEnumType(Likelihood) },
    { no: 12, name: "surprise_likelihood", kind: "enum", T: proto3.getEnumType(Likelihood) },
    { no: 13, name: "under_exposed_likelihood", kind: "enum", T: proto3.getEnumType(Likelihood) },
    { no: 14, name: "blurred_likelihood", kind: "enum", T: proto3.getEnumType(Likelihood) },
    { no: 15, name: "headwear_likelihood", kind: "enum", T: proto3.getEnumType(Likelihood) },
  ]);

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): FaceAnnotation {
    return new FaceAnnotation().fromBinary(bytes, options);
  }

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): FaceAnnotation {
    return new FaceAnnotation().fromJson(jsonValue, options);
  }

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): FaceAnnotation {
    return new FaceAnnotation().fromJsonString(jsonString, options);
  }

  static equals(a: FaceAnnotation | PlainMessage<FaceAnnotation> | undefined, b: FaceAnnotation | PlainMessage<FaceAnnotation> | undefined): boolean {
    return proto3.util.equals(FaceAnnotation, a, b);
  }
}

/**
 * A face-specific landmark (for example, a face feature).
 * Landmark positions may fall outside the bounds of the image
 * if the face is near one or more edges of the image.
 * Therefore it is NOT guaranteed that `0 <= x < width` or
 * `0 <= y < height`.
 *
 * @generated from message google.cloud.vision.v1.FaceAnnotation.Landmark
 */
export class FaceAnnotation_Landmark extends Message<FaceAnnotation_Landmark> {
  /**
   * Face landmark type.
   *
   * @generated from field: google.cloud.vision.v1.FaceAnnotation.Landmark.Type type = 3;
   */
  type = FaceAnnotation_Landmark_Type.UNKNOWN_LANDMARK;

  /**
   * Face landmark position.
   *
   * @generated from field: google.cloud.vision.v1.Position position = 4;
   */
  position?: Position;

  constructor(data?: PartialMessage<FaceAnnotation_Landmark>) {
    super();
    proto3.util.initPartial(data, this);
  }

  static readonly runtime: typeof proto3 = proto3;
  static readonly typeName = "google.cloud.vision.v1.FaceAnnotation.Landmark";
  static readonly fields: FieldList = proto3.util.newFieldList(() => [
    { no: 3, name: "type", kind: "enum", T: proto3.getEnumType(FaceAnnotation_Landmark_Type) },
    { no: 4, name: "position", kind: "message", T: Position },
  ]);

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): FaceAnnotation_Landmark {
    return new FaceAnnotation_Landmark().fromBinary(bytes, options);
  }

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): FaceAnnotation_Landmark {
    return new FaceAnnotation_Landmark().fromJson(jsonValue, options);
  }

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): FaceAnnotation_Landmark {
    return new FaceAnnotation_Landmark().fromJsonString(jsonString, options);
  }

  static equals(a: FaceAnnotation_Landmark | PlainMessage<FaceAnnotation_Landmark> | undefined, b: FaceAnnotation_Landmark | PlainMessage<FaceAnnotation_Landmark> | undefined): boolean {
    return proto3.util.equals(FaceAnnotation_Landmark, a, b);
  }
}

/**
 * Face landmark (feature) type.
 * Left and right are defined from the vantage of the viewer of the image
 * without considering mirror projections typical of photos. So, `LEFT_EYE`,
 * typically, is the person's right eye.
 *
 * @generated from enum google.cloud.vision.v1.FaceAnnotation.Landmark.Type
 */
export enum FaceAnnotation_Landmark_Type {
  /**
   * Unknown face landmark detected. Should not be filled.
   *
   * @generated from enum value: UNKNOWN_LANDMARK = 0;
   */
  UNKNOWN_LANDMARK = 0,

  /**
   * Left eye.
   *
   * @generated from enum value: LEFT_EYE = 1;
   */
  LEFT_EYE = 1,

  /**
   * Right eye.
   *
   * @generated from enum value: RIGHT_EYE = 2;
   */
  RIGHT_EYE = 2,

  /**
   * Left of left eyebrow.
   *
   * @generated from enum value: LEFT_OF_LEFT_EYEBROW = 3;
   */
  LEFT_OF_LEFT_EYEBROW = 3,

  /**
   * Right of left eyebrow.
   *
   * @generated from enum value: RIGHT_OF_LEFT_EYEBROW = 4;
   */
  RIGHT_OF_LEFT_EYEBROW = 4,

  /**
   * Left of right eyebrow.
   *
   * @generated from enum value: LEFT_OF_RIGHT_EYEBROW = 5;
   */
  LEFT_OF_RIGHT_EYEBROW = 5,

  /**
   * Right of right eyebrow.
   *
   * @generated from enum value: RIGHT_OF_RIGHT_EYEBROW = 6;
   */
  RIGHT_OF_RIGHT_EYEBROW = 6,

  /**
   * Midpoint between eyes.
   *
   * @generated from enum value: MIDPOINT_BETWEEN_EYES = 7;
   */
  MIDPOINT_BETWEEN_EYES = 7,

  /**
   * Nose tip.
   *
   * @generated from enum value: NOSE_TIP = 8;
   */
  NOSE_TIP = 8,

  /**
   * Upper lip.
   *
   * @generated from enum value: UPPER_LIP = 9;
   */
  UPPER_LIP = 9,

  /**
   * Lower lip.
   *
   * @generated from enum value: LOWER_LIP = 10;
   */
  LOWER_LIP = 10,

  /**
   * Mouth left.
   *
   * @generated from enum value: MOUTH_LEFT = 11;
   */
  MOUTH_LEFT = 11,

  /**
   * Mouth right.
   *
   * @generated from enum value: MOUTH_RIGHT = 12;
   */
  MOUTH_RIGHT = 12,

  /**
   * Mouth center.
   *
   * @generated from enum value: MOUTH_CENTER = 13;
   */
  MOUTH_CENTER = 13,

  /**
   * Nose, bottom right.
   *
   * @generated from enum value: NOSE_BOTTOM_RIGHT = 14;
   */
  NOSE_BOTTOM_RIGHT = 14,

  /**
   * Nose, bottom left.
   *
   * @generated from enum value: NOSE_BOTTOM_LEFT = 15;
   */
  NOSE_BOTTOM_LEFT = 15,

  /**
   * Nose, bottom center.
   *
   * @generated from enum value: NOSE_BOTTOM_CENTER = 16;
   */
  NOSE_BOTTOM_CENTER = 16,

  /**
   * Left eye, top boundary.
   *
   * @generated from enum value: LEFT_EYE_TOP_BOUNDARY = 17;
   */
  LEFT_EYE_TOP_BOUNDARY = 17,

  /**
   * Left eye, right corner.
   *
   * @generated from enum value: LEFT_EYE_RIGHT_CORNER = 18;
   */
  LEFT_EYE_RIGHT_CORNER = 18,

  /**
   * Left eye, bottom boundary.
   *
   * @generated from enum value: LEFT_EYE_BOTTOM_BOUNDARY = 19;
   */
  LEFT_EYE_BOTTOM_BOUNDARY = 19,

  /**
   * Left eye, left corner.
   *
   * @generated from enum value: LEFT_EYE_LEFT_CORNER = 20;
   */
  LEFT_EYE_LEFT_CORNER = 20,

  /**
   * Right eye, top boundary.
   *
   * @generated from enum value: RIGHT_EYE_TOP_BOUNDARY = 21;
   */
  RIGHT_EYE_TOP_BOUNDARY = 21,

  /**
   * Right eye, right corner.
   *
   * @generated from enum value: RIGHT_EYE_RIGHT_CORNER = 22;
   */
  RIGHT_EYE_RIGHT_CORNER = 22,

  /**
   * Right eye, bottom boundary.
   *
   * @generated from enum value: RIGHT_EYE_BOTTOM_BOUNDARY = 23;
   */
  RIGHT_EYE_BOTTOM_BOUNDARY = 23,

  /**
   * Right eye, left corner.
   *
   * @generated from enum value: RIGHT_EYE_LEFT_CORNER = 24;
   */
  RIGHT_EYE_LEFT_CORNER = 24,

  /**
   * Left eyebrow, upper midpoint.
   *
   * @generated from enum value: LEFT_EYEBROW_UPPER_MIDPOINT = 25;
   */
  LEFT_EYEBROW_UPPER_MIDPOINT = 25,

  /**
   * Right eyebrow, upper midpoint.
   *
   * @generated from enum value: RIGHT_EYEBROW_UPPER_MIDPOINT = 26;
   */
  RIGHT_EYEBROW_UPPER_MIDPOINT = 26,

  /**
   * Left ear tragion.
   *
   * @generated from enum value: LEFT_EAR_TRAGION = 27;
   */
  LEFT_EAR_TRAGION = 27,

  /**
   * Right ear tragion.
   *
   * @generated from enum value: RIGHT_EAR_TRAGION = 28;
   */
  RIGHT_EAR_TRAGION = 28,

  /**
   * Left eye pupil.
   *
   * @generated from enum value: LEFT_EYE_PUPIL = 29;
   */
  LEFT_EYE_PUPIL = 29,

  /**
   * Right eye pupil.
   *
   * @generated from enum value: RIGHT_EYE_PUPIL = 30;
   */
  RIGHT_EYE_PUPIL = 30,

  /**
   * Forehead glabella.
   *
   * @generated from enum value: FOREHEAD_GLABELLA = 31;
   */
  FOREHEAD_GLABELLA = 31,

  /**
   * Chin gnathion.
   *
   * @generated from enum value: CHIN_GNATHION = 32;
   */
  CHIN_GNATHION = 32,

  /**
   * Chin left gonion.
   *
   * @generated from enum value: CHIN_LEFT_GONION = 33;
   */
  CHIN_LEFT_GONION = 33,

  /**
   * Chin right gonion.
   *
   * @generated from enum value: CHIN_RIGHT_GONION = 34;
   */
  CHIN_RIGHT_GONION = 34,
}
// Retrieve enum metadata with: proto3.getEnumType(FaceAnnotation_Landmark_Type)
proto3.util.setEnumType(FaceAnnotation_Landmark_Type, "google.cloud.vision.v1.FaceAnnotation.Landmark.Type", [
  { no: 0, name: "UNKNOWN_LANDMARK" },
  { no: 1, name: "LEFT_EYE" },
  { no: 2, name: "RIGHT_EYE" },
  { no: 3, name: "LEFT_OF_LEFT_EYEBROW" },
  { no: 4, name: "RIGHT_OF_LEFT_EYEBROW" },
  { no: 5, name: "LEFT_OF_RIGHT_EYEBROW" },
  { no: 6, name: "RIGHT_OF_RIGHT_EYEBROW" },
  { no: 7, name: "MIDPOINT_BETWEEN_EYES" },
  { no: 8, name: "NOSE_TIP" },
  { no: 9, name: "UPPER_LIP" },
  { no: 10, name: "LOWER_LIP" },
  { no: 11, name: "MOUTH_LEFT" },
  { no: 12, name: "MOUTH_RIGHT" },
  { no: 13, name: "MOUTH_CENTER" },
  { no: 14, name: "NOSE_BOTTOM_RIGHT" },
  { no: 15, name: "NOSE_BOTTOM_LEFT" },
  { no: 16, name: "NOSE_BOTTOM_CENTER" },
  { no: 17, name: "LEFT_EYE_TOP_BOUNDARY" },
  { no: 18, name: "LEFT_EYE_RIGHT_CORNER" },
  { no: 19, name: "LEFT_EYE_BOTTOM_BOUNDARY" },
  { no: 20, name: "LEFT_EYE_LEFT_CORNER" },
  { no: 21, name: "RIGHT_EYE_TOP_BOUNDARY" },
  { no: 22, name: "RIGHT_EYE_RIGHT_CORNER" },
  { no: 23, name: "RIGHT_EYE_BOTTOM_BOUNDARY" },
  { no: 24, name: "RIGHT_EYE_LEFT_CORNER" },
  { no: 25, name: "LEFT_EYEBROW_UPPER_MIDPOINT" },
  { no: 26, name: "RIGHT_EYEBROW_UPPER_MIDPOINT" },
  { no: 27, name: "LEFT_EAR_TRAGION" },
  { no: 28, name: "RIGHT_EAR_TRAGION" },
  { no: 29, name: "LEFT_EYE_PUPIL" },
  { no: 30, name: "RIGHT_EYE_PUPIL" },
  { no: 31, name: "FOREHEAD_GLABELLA" },
  { no: 32, name: "CHIN_GNATHION" },
  { no: 33, name: "CHIN_LEFT_GONION" },
  { no: 34, name: "CHIN_RIGHT_GONION" },
]);

/**
 * Detected entity location information.
 *
 * @generated from message google.cloud.vision.v1.LocationInfo
 */
export class LocationInfo extends Message<LocationInfo> {
  /**
   * lat/long location coordinates.
   *
   * @generated from field: google.type.LatLng lat_lng = 1;
   */
  latLng?: LatLng;

  constructor(data?: PartialMessage<LocationInfo>) {
    super();
    proto3.util.initPartial(data, this);
  }

  static readonly runtime: typeof proto3 = proto3;
  static readonly typeName = "google.cloud.vision.v1.LocationInfo";
  static readonly fields: FieldList = proto3.util.newFieldList(() => [
    { no: 1, name: "lat_lng", kind: "message", T: LatLng },
  ]);

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): LocationInfo {
    return new LocationInfo().fromBinary(bytes, options);
  }

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): LocationInfo {
    return new LocationInfo().fromJson(jsonValue, options);
  }

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): LocationInfo {
    return new LocationInfo().fromJsonString(jsonString, options);
  }

  static equals(a: LocationInfo | PlainMessage<LocationInfo> | undefined, b: LocationInfo | PlainMessage<LocationInfo> | undefined): boolean {
    return proto3.util.equals(LocationInfo, a, b);
  }
}

/**
 * A `Property` consists of a user-supplied name/value pair.
 *
 * @generated from message google.cloud.vision.v1.Property
 */
export class Property extends Message<Property> {
  /**
   * Name of the property.
   *
   * @generated from field: string name = 1;
   */
  name = "";

  /**
   * Value of the property.
   *
   * @generated from field: string value = 2;
   */
  value = "";

  constructor(data?: PartialMessage<Property>) {
    super();
    proto3.util.initPartial(data, this);
  }

  static readonly runtime: typeof proto3 = proto3;
  static readonly typeName = "google.cloud.vision.v1.Property";
  static readonly fields: FieldList = proto3.util.newFieldList(() => [
    { no: 1, name: "name", kind: "scalar", T: 9 /* ScalarType.STRING */ },
    { no: 2, name: "value", kind: "scalar", T: 9 /* ScalarType.STRING */ },
  ]);

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): Property {
    return new Property().fromBinary(bytes, options);
  }

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): Property {
    return new Property().fromJson(jsonValue, options);
  }

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): Property {
    return new Property().fromJsonString(jsonString, options);
  }

  static equals(a: Property | PlainMessage<Property> | undefined, b: Property | PlainMessage<Property> | undefined): boolean {
    return proto3.util.equals(Property, a, b);
  }
}

/**
 * Set of detected entity features.
 *
 * @generated from message google.cloud.vision.v1.EntityAnnotation
 */
export class EntityAnnotation extends Message<EntityAnnotation> {
  /**
   * Opaque entity ID. Some IDs may be available in
   * [Google Knowledge Graph Search API](https://developers.google.com/knowledge-graph/).
   *
   * @generated from field: string mid = 1;
   */
  mid = "";

  /**
   * The language code for the locale in which the entity textual
   * `description` is expressed.
   *
   * @generated from field: string locale = 2;
   */
  locale = "";

  /**
   * Entity textual description, expressed in its `locale` language.
   *
   * @generated from field: string description = 3;
   */
  description = "";

  /**
   * Overall score of the result. Range [0, 1].
   *
   * @generated from field: float score = 4;
   */
  score = 0;

  /**
   * The accuracy of the entity detection in an image.
   * For example, for an image in which the "Eiffel Tower" entity is detected,
   * this field represents the confidence that there is a tower in the query
   * image. Range [0, 1].
   *
   * @generated from field: float confidence = 5;
   */
  confidence = 0;

  /**
   * The relevancy of the ICA (Image Content Annotation) label to the
   * image. For example, the relevancy of "tower" is likely higher to an image
   * containing the detected "Eiffel Tower" than to an image containing a
   * detected distant towering building, even though the confidence that
   * there is a tower in each image may be the same. Range [0, 1].
   *
   * @generated from field: float topicality = 6;
   */
  topicality = 0;

  /**
   * Image region to which this entity belongs. Currently not produced
   * for `LABEL_DETECTION` features. For `TEXT_DETECTION` (OCR), `boundingPoly`s
   * are produced for the entire text detected in an image region, followed by
   * `boundingPoly`s for each word within the detected text.
   *
   * @generated from field: google.cloud.vision.v1.BoundingPoly bounding_poly = 7;
   */
  boundingPoly?: BoundingPoly;

  /**
   * The location information for the detected entity. Multiple
   * `LocationInfo` elements can be present because one location may
   * indicate the location of the scene in the image, and another location
   * may indicate the location of the place where the image was taken.
   * Location information is usually present for landmarks.
   *
   * @generated from field: repeated google.cloud.vision.v1.LocationInfo locations = 8;
   */
  locations: LocationInfo[] = [];

  /**
   * Some entities may have optional user-supplied `Property` (name/value)
   * fields, such a score or string that qualifies the entity.
   *
   * @generated from field: repeated google.cloud.vision.v1.Property properties = 9;
   */
  properties: Property[] = [];

  constructor(data?: PartialMessage<EntityAnnotation>) {
    super();
    proto3.util.initPartial(data, this);
  }

  static readonly runtime: typeof proto3 = proto3;
  static readonly typeName = "google.cloud.vision.v1.EntityAnnotation";
  static readonly fields: FieldList = proto3.util.newFieldList(() => [
    { no: 1, name: "mid", kind: "scalar", T: 9 /* ScalarType.STRING */ },
    { no: 2, name: "locale", kind: "scalar", T: 9 /* ScalarType.STRING */ },
    { no: 3, name: "description", kind: "scalar", T: 9 /* ScalarType.STRING */ },
    { no: 4, name: "score", kind: "scalar", T: 2 /* ScalarType.FLOAT */ },
    { no: 5, name: "confidence", kind: "scalar", T: 2 /* ScalarType.FLOAT */ },
    { no: 6, name: "topicality", kind: "scalar", T: 2 /* ScalarType.FLOAT */ },
    { no: 7, name: "bounding_poly", kind: "message", T: BoundingPoly },
    { no: 8, name: "locations", kind: "message", T: LocationInfo, repeated: true },
    { no: 9, name: "properties", kind: "message", T: Property, repeated: true },
  ]);

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): EntityAnnotation {
    return new EntityAnnotation().fromBinary(bytes, options);
  }

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): EntityAnnotation {
    return new EntityAnnotation().fromJson(jsonValue, options);
  }

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): EntityAnnotation {
    return new EntityAnnotation().fromJsonString(jsonString, options);
  }

  static equals(a: EntityAnnotation | PlainMessage<EntityAnnotation> | undefined, b: EntityAnnotation | PlainMessage<EntityAnnotation> | undefined): boolean {
    return proto3.util.equals(EntityAnnotation, a, b);
  }
}

/**
 * Set of features pertaining to the image, computed by computer vision
 * methods over safe-search verticals (for example, adult, spoof, medical,
 * violence).
 *
 * @generated from message google.cloud.vision.v1.SafeSearchAnnotation
 */
export class SafeSearchAnnotation extends Message<SafeSearchAnnotation> {
  /**
   * Represents the adult content likelihood for the image.
   *
   * @generated from field: google.cloud.vision.v1.Likelihood adult = 1;
   */
  adult = Likelihood.UNKNOWN;

  /**
   * Spoof likelihood. The likelihood that an modification
   * was made to the image's canonical version to make it appear
   * funny or offensive.
   *
   * @generated from field: google.cloud.vision.v1.Likelihood spoof = 2;
   */
  spoof = Likelihood.UNKNOWN;

  /**
   * Likelihood that this is a medical image.
   *
   * @generated from field: google.cloud.vision.v1.Likelihood medical = 3;
   */
  medical = Likelihood.UNKNOWN;

  /**
   * Violence likelihood.
   *
   * @generated from field: google.cloud.vision.v1.Likelihood violence = 4;
   */
  violence = Likelihood.UNKNOWN;

  constructor(data?: PartialMessage<SafeSearchAnnotation>) {
    super();
    proto3.util.initPartial(data, this);
  }

  static readonly runtime: typeof proto3 = proto3;
  static readonly typeName = "google.cloud.vision.v1.SafeSearchAnnotation";
  static readonly fields: FieldList = proto3.util.newFieldList(() => [
    { no: 1, name: "adult", kind: "enum", T: proto3.getEnumType(Likelihood) },
    { no: 2, name: "spoof", kind: "enum", T: proto3.getEnumType(Likelihood) },
    { no: 3, name: "medical", kind: "enum", T: proto3.getEnumType(Likelihood) },
    { no: 4, name: "violence", kind: "enum", T: proto3.getEnumType(Likelihood) },
  ]);

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): SafeSearchAnnotation {
    return new SafeSearchAnnotation().fromBinary(bytes, options);
  }

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): SafeSearchAnnotation {
    return new SafeSearchAnnotation().fromJson(jsonValue, options);
  }

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): SafeSearchAnnotation {
    return new SafeSearchAnnotation().fromJsonString(jsonString, options);
  }

  static equals(a: SafeSearchAnnotation | PlainMessage<SafeSearchAnnotation> | undefined, b: SafeSearchAnnotation | PlainMessage<SafeSearchAnnotation> | undefined): boolean {
    return proto3.util.equals(SafeSearchAnnotation, a, b);
  }
}

/**
 * Rectangle determined by min and max `LatLng` pairs.
 *
 * @generated from message google.cloud.vision.v1.LatLongRect
 */
export class LatLongRect extends Message<LatLongRect> {
  /**
   * Min lat/long pair.
   *
   * @generated from field: google.type.LatLng min_lat_lng = 1;
   */
  minLatLng?: LatLng;

  /**
   * Max lat/long pair.
   *
   * @generated from field: google.type.LatLng max_lat_lng = 2;
   */
  maxLatLng?: LatLng;

  constructor(data?: PartialMessage<LatLongRect>) {
    super();
    proto3.util.initPartial(data, this);
  }

  static readonly runtime: typeof proto3 = proto3;
  static readonly typeName = "google.cloud.vision.v1.LatLongRect";
  static readonly fields: FieldList = proto3.util.newFieldList(() => [
    { no: 1, name: "min_lat_lng", kind: "message", T: LatLng },
    { no: 2, name: "max_lat_lng", kind: "message", T: LatLng },
  ]);

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): LatLongRect {
    return new LatLongRect().fromBinary(bytes, options);
  }

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): LatLongRect {
    return new LatLongRect().fromJson(jsonValue, options);
  }

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): LatLongRect {
    return new LatLongRect().fromJsonString(jsonString, options);
  }

  static equals(a: LatLongRect | PlainMessage<LatLongRect> | undefined, b: LatLongRect | PlainMessage<LatLongRect> | undefined): boolean {
    return proto3.util.equals(LatLongRect, a, b);
  }
}

/**
 * Color information consists of RGB channels, score, and the fraction of
 * the image that the color occupies in the image.
 *
 * @generated from message google.cloud.vision.v1.ColorInfo
 */
export class ColorInfo extends Message<ColorInfo> {
  /**
   * RGB components of the color.
   *
   * @generated from field: google.type.Color color = 1;
   */
  color?: Color;

  /**
   * Image-specific score for this color. Value in range [0, 1].
   *
   * @generated from field: float score = 2;
   */
  score = 0;

  /**
   * The fraction of pixels the color occupies in the image.
   * Value in range [0, 1].
   *
   * @generated from field: float pixel_fraction = 3;
   */
  pixelFraction = 0;

  constructor(data?: PartialMessage<ColorInfo>) {
    super();
    proto3.util.initPartial(data, this);
  }

  static readonly runtime: typeof proto3 = proto3;
  static readonly typeName = "google.cloud.vision.v1.ColorInfo";
  static readonly fields: FieldList = proto3.util.newFieldList(() => [
    { no: 1, name: "color", kind: "message", T: Color },
    { no: 2, name: "score", kind: "scalar", T: 2 /* ScalarType.FLOAT */ },
    { no: 3, name: "pixel_fraction", kind: "scalar", T: 2 /* ScalarType.FLOAT */ },
  ]);

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): ColorInfo {
    return new ColorInfo().fromBinary(bytes, options);
  }

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): ColorInfo {
    return new ColorInfo().fromJson(jsonValue, options);
  }

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): ColorInfo {
    return new ColorInfo().fromJsonString(jsonString, options);
  }

  static equals(a: ColorInfo | PlainMessage<ColorInfo> | undefined, b: ColorInfo | PlainMessage<ColorInfo> | undefined): boolean {
    return proto3.util.equals(ColorInfo, a, b);
  }
}

/**
 * Set of dominant colors and their corresponding scores.
 *
 * @generated from message google.cloud.vision.v1.DominantColorsAnnotation
 */
export class DominantColorsAnnotation extends Message<DominantColorsAnnotation> {
  /**
   * RGB color values with their score and pixel fraction.
   *
   * @generated from field: repeated google.cloud.vision.v1.ColorInfo colors = 1;
   */
  colors: ColorInfo[] = [];

  constructor(data?: PartialMessage<DominantColorsAnnotation>) {
    super();
    proto3.util.initPartial(data, this);
  }

  static readonly runtime: typeof proto3 = proto3;
  static readonly typeName = "google.cloud.vision.v1.DominantColorsAnnotation";
  static readonly fields: FieldList = proto3.util.newFieldList(() => [
    { no: 1, name: "colors", kind: "message", T: ColorInfo, repeated: true },
  ]);

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): DominantColorsAnnotation {
    return new DominantColorsAnnotation().fromBinary(bytes, options);
  }

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): DominantColorsAnnotation {
    return new DominantColorsAnnotation().fromJson(jsonValue, options);
  }

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): DominantColorsAnnotation {
    return new DominantColorsAnnotation().fromJsonString(jsonString, options);
  }

  static equals(a: DominantColorsAnnotation | PlainMessage<DominantColorsAnnotation> | undefined, b: DominantColorsAnnotation | PlainMessage<DominantColorsAnnotation> | undefined): boolean {
    return proto3.util.equals(DominantColorsAnnotation, a, b);
  }
}

/**
 * Stores image properties, such as dominant colors.
 *
 * @generated from message google.cloud.vision.v1.ImageProperties
 */
export class ImageProperties extends Message<ImageProperties> {
  /**
   * If present, dominant colors completed successfully.
   *
   * @generated from field: google.cloud.vision.v1.DominantColorsAnnotation dominant_colors = 1;
   */
  dominantColors?: DominantColorsAnnotation;

  constructor(data?: PartialMessage<ImageProperties>) {
    super();
    proto3.util.initPartial(data, this);
  }

  static readonly runtime: typeof proto3 = proto3;
  static readonly typeName = "google.cloud.vision.v1.ImageProperties";
  static readonly fields: FieldList = proto3.util.newFieldList(() => [
    { no: 1, name: "dominant_colors", kind: "message", T: DominantColorsAnnotation },
  ]);

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): ImageProperties {
    return new ImageProperties().fromBinary(bytes, options);
  }

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): ImageProperties {
    return new ImageProperties().fromJson(jsonValue, options);
  }

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): ImageProperties {
    return new ImageProperties().fromJsonString(jsonString, options);
  }

  static equals(a: ImageProperties | PlainMessage<ImageProperties> | undefined, b: ImageProperties | PlainMessage<ImageProperties> | undefined): boolean {
    return proto3.util.equals(ImageProperties, a, b);
  }
}

/**
 * Single crop hint that is used to generate a new crop when serving an image.
 *
 * @generated from message google.cloud.vision.v1.CropHint
 */
export class CropHint extends Message<CropHint> {
  /**
   * The bounding polygon for the crop region. The coordinates of the bounding
   * box are in the original image's scale, as returned in `ImageParams`.
   *
   * @generated from field: google.cloud.vision.v1.BoundingPoly bounding_poly = 1;
   */
  boundingPoly?: BoundingPoly;

  /**
   * Confidence of this being a salient region.  Range [0, 1].
   *
   * @generated from field: float confidence = 2;
   */
  confidence = 0;

  /**
   * Fraction of importance of this salient region with respect to the original
   * image.
   *
   * @generated from field: float importance_fraction = 3;
   */
  importanceFraction = 0;

  constructor(data?: PartialMessage<CropHint>) {
    super();
    proto3.util.initPartial(data, this);
  }

  static readonly runtime: typeof proto3 = proto3;
  static readonly typeName = "google.cloud.vision.v1.CropHint";
  static readonly fields: FieldList = proto3.util.newFieldList(() => [
    { no: 1, name: "bounding_poly", kind: "message", T: BoundingPoly },
    { no: 2, name: "confidence", kind: "scalar", T: 2 /* ScalarType.FLOAT */ },
    { no: 3, name: "importance_fraction", kind: "scalar", T: 2 /* ScalarType.FLOAT */ },
  ]);

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): CropHint {
    return new CropHint().fromBinary(bytes, options);
  }

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): CropHint {
    return new CropHint().fromJson(jsonValue, options);
  }

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): CropHint {
    return new CropHint().fromJsonString(jsonString, options);
  }

  static equals(a: CropHint | PlainMessage<CropHint> | undefined, b: CropHint | PlainMessage<CropHint> | undefined): boolean {
    return proto3.util.equals(CropHint, a, b);
  }
}

/**
 * Set of crop hints that are used to generate new crops when serving images.
 *
 * @generated from message google.cloud.vision.v1.CropHintsAnnotation
 */
export class CropHintsAnnotation extends Message<CropHintsAnnotation> {
  /**
   * @generated from field: repeated google.cloud.vision.v1.CropHint crop_hints = 1;
   */
  cropHints: CropHint[] = [];

  constructor(data?: PartialMessage<CropHintsAnnotation>) {
    super();
    proto3.util.initPartial(data, this);
  }

  static readonly runtime: typeof proto3 = proto3;
  static readonly typeName = "google.cloud.vision.v1.CropHintsAnnotation";
  static readonly fields: FieldList = proto3.util.newFieldList(() => [
    { no: 1, name: "crop_hints", kind: "message", T: CropHint, repeated: true },
  ]);

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): CropHintsAnnotation {
    return new CropHintsAnnotation().fromBinary(bytes, options);
  }

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): CropHintsAnnotation {
    return new CropHintsAnnotation().fromJson(jsonValue, options);
  }

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): CropHintsAnnotation {
    return new CropHintsAnnotation().fromJsonString(jsonString, options);
  }

  static equals(a: CropHintsAnnotation | PlainMessage<CropHintsAnnotation> | undefined, b: CropHintsAnnotation | PlainMessage<CropHintsAnnotation> | undefined): boolean {
    return proto3.util.equals(CropHintsAnnotation, a, b);
  }
}

/**
 * Parameters for crop hints annotation request.
 *
 * @generated from message google.cloud.vision.v1.CropHintsParams
 */
export class CropHintsParams extends Message<CropHintsParams> {
  /**
   * Aspect ratios in floats, representing the ratio of the width to the height
   * of the image. For example, if the desired aspect ratio is 4/3, the
   * corresponding float value should be 1.33333.  If not specified, the
   * best possible crop is returned. The number of provided aspect ratios is
   * limited to a maximum of 16; any aspect ratios provided after the 16th are
   * ignored.
   *
   * @generated from field: repeated float aspect_ratios = 1;
   */
  aspectRatios: number[] = [];

  constructor(data?: PartialMessage<CropHintsParams>) {
    super();
    proto3.util.initPartial(data, this);
  }

  static readonly runtime: typeof proto3 = proto3;
  static readonly typeName = "google.cloud.vision.v1.CropHintsParams";
  static readonly fields: FieldList = proto3.util.newFieldList(() => [
    { no: 1, name: "aspect_ratios", kind: "scalar", T: 2 /* ScalarType.FLOAT */, repeated: true },
  ]);

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): CropHintsParams {
    return new CropHintsParams().fromBinary(bytes, options);
  }

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): CropHintsParams {
    return new CropHintsParams().fromJson(jsonValue, options);
  }

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): CropHintsParams {
    return new CropHintsParams().fromJsonString(jsonString, options);
  }

  static equals(a: CropHintsParams | PlainMessage<CropHintsParams> | undefined, b: CropHintsParams | PlainMessage<CropHintsParams> | undefined): boolean {
    return proto3.util.equals(CropHintsParams, a, b);
  }
}

/**
 * Image context and/or feature-specific parameters.
 *
 * @generated from message google.cloud.vision.v1.ImageContext
 */
export class ImageContext extends Message<ImageContext> {
  /**
   * lat/long rectangle that specifies the location of the image.
   *
   * @generated from field: google.cloud.vision.v1.LatLongRect lat_long_rect = 1;
   */
  latLongRect?: LatLongRect;

  /**
   * List of languages to use for TEXT_DETECTION. In most cases, an empty value
   * yields the best results since it enables automatic language detection. For
   * languages based on the Latin alphabet, setting `language_hints` is not
   * needed. In rare cases, when the language of the text in the image is known,
   * setting a hint will help get better results (although it will be a
   * significant hindrance if the hint is wrong). Text detection returns an
   * error if one or more of the specified languages is not one of the
   * [supported languages](/vision/docs/languages).
   *
   * @generated from field: repeated string language_hints = 2;
   */
  languageHints: string[] = [];

  /**
   * Parameters for crop hints annotation request.
   *
   * @generated from field: google.cloud.vision.v1.CropHintsParams crop_hints_params = 4;
   */
  cropHintsParams?: CropHintsParams;

  constructor(data?: PartialMessage<ImageContext>) {
    super();
    proto3.util.initPartial(data, this);
  }

  static readonly runtime: typeof proto3 = proto3;
  static readonly typeName = "google.cloud.vision.v1.ImageContext";
  static readonly fields: FieldList = proto3.util.newFieldList(() => [
    { no: 1, name: "lat_long_rect", kind: "message", T: LatLongRect },
    { no: 2, name: "language_hints", kind: "scalar", T: 9 /* ScalarType.STRING */, repeated: true },
    { no: 4, name: "crop_hints_params", kind: "message", T: CropHintsParams },
  ]);

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): ImageContext {
    return new ImageContext().fromBinary(bytes, options);
  }

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): ImageContext {
    return new ImageContext().fromJson(jsonValue, options);
  }

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): ImageContext {
    return new ImageContext().fromJsonString(jsonString, options);
  }

  static equals(a: ImageContext | PlainMessage<ImageContext> | undefined, b: ImageContext | PlainMessage<ImageContext> | undefined): boolean {
    return proto3.util.equals(ImageContext, a, b);
  }
}

/**
 * Request for performing Google Cloud Vision API tasks over a user-provided
 * image, with user-requested features.
 *
 * @generated from message google.cloud.vision.v1.AnnotateImageRequest
 */
export class AnnotateImageRequest extends Message<AnnotateImageRequest> {
  /**
   * The image to be processed.
   *
   * @generated from field: google.cloud.vision.v1.Image image = 1;
   */
  image?: Image;

  /**
   * Requested features.
   *
   * @generated from field: repeated google.cloud.vision.v1.Feature features = 2;
   */
  features: Feature[] = [];

  /**
   * Additional context that may accompany the image.
   *
   * @generated from field: google.cloud.vision.v1.ImageContext image_context = 3;
   */
  imageContext?: ImageContext;

  constructor(data?: PartialMessage<AnnotateImageRequest>) {
    super();
    proto3.util.initPartial(data, this);
  }

  static readonly runtime: typeof proto3 = proto3;
  static readonly typeName = "google.cloud.vision.v1.AnnotateImageRequest";
  static readonly fields: FieldList = proto3.util.newFieldList(() => [
    { no: 1, name: "image", kind: "message", T: Image },
    { no: 2, name: "features", kind: "message", T: Feature, repeated: true },
    { no: 3, name: "image_context", kind: "message", T: ImageContext },
  ]);

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): AnnotateImageRequest {
    return new AnnotateImageRequest().fromBinary(bytes, options);
  }

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): AnnotateImageRequest {
    return new AnnotateImageRequest().fromJson(jsonValue, options);
  }

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): AnnotateImageRequest {
    return new AnnotateImageRequest().fromJsonString(jsonString, options);
  }

  static equals(a: AnnotateImageRequest | PlainMessage<AnnotateImageRequest> | undefined, b: AnnotateImageRequest | PlainMessage<AnnotateImageRequest> | undefined): boolean {
    return proto3.util.equals(AnnotateImageRequest, a, b);
  }
}

/**
 * Response to an image annotation request.
 *
 * @generated from message google.cloud.vision.v1.AnnotateImageResponse
 */
export class AnnotateImageResponse extends Message<AnnotateImageResponse> {
  /**
   * If present, face detection has completed successfully.
   *
   * @generated from field: repeated google.cloud.vision.v1.FaceAnnotation face_annotations = 1;
   */
  faceAnnotations: FaceAnnotation[] = [];

  /**
   * If present, landmark detection has completed successfully.
   *
   * @generated from field: repeated google.cloud.vision.v1.EntityAnnotation landmark_annotations = 2;
   */
  landmarkAnnotations: EntityAnnotation[] = [];

  /**
   * If present, logo detection has completed successfully.
   *
   * @generated from field: repeated google.cloud.vision.v1.EntityAnnotation logo_annotations = 3;
   */
  logoAnnotations: EntityAnnotation[] = [];

  /**
   * If present, label detection has completed successfully.
   *
   * @generated from field: repeated google.cloud.vision.v1.EntityAnnotation label_annotations = 4;
   */
  labelAnnotations: EntityAnnotation[] = [];

  /**
   * If present, text (OCR) detection or document (OCR) text detection has
   * completed successfully.
   *
   * @generated from field: repeated google.cloud.vision.v1.EntityAnnotation text_annotations = 5;
   */
  textAnnotations: EntityAnnotation[] = [];

  /**
   * If present, text (OCR) detection or document (OCR) text detection has
   * completed successfully.
   * This annotation provides the structural hierarchy for the OCR detected
   * text.
   *
   * @generated from field: google.cloud.vision.v1.TextAnnotation full_text_annotation = 12;
   */
  fullTextAnnotation?: TextAnnotation;

  /**
   * If present, safe-search annotation has completed successfully.
   *
   * @generated from field: google.cloud.vision.v1.SafeSearchAnnotation safe_search_annotation = 6;
   */
  safeSearchAnnotation?: SafeSearchAnnotation;

  /**
   * If present, image properties were extracted successfully.
   *
   * @generated from field: google.cloud.vision.v1.ImageProperties image_properties_annotation = 8;
   */
  imagePropertiesAnnotation?: ImageProperties;

  /**
   * If present, crop hints have completed successfully.
   *
   * @generated from field: google.cloud.vision.v1.CropHintsAnnotation crop_hints_annotation = 11;
   */
  cropHintsAnnotation?: CropHintsAnnotation;

  /**
   * If present, web detection has completed successfully.
   *
   * @generated from field: google.cloud.vision.v1.WebDetection web_detection = 13;
   */
  webDetection?: WebDetection;

  /**
   * If set, represents the error message for the operation.
   * Note that filled-in image annotations are guaranteed to be
   * correct, even when `error` is set.
   *
   * @generated from field: google.rpc.Status error = 9;
   */
  error?: Status;

  constructor(data?: PartialMessage<AnnotateImageResponse>) {
    super();
    proto3.util.initPartial(data, this);
  }

  static readonly runtime: typeof proto3 = proto3;
  static readonly typeName = "google.cloud.vision.v1.AnnotateImageResponse";
  static readonly fields: FieldList = proto3.util.newFieldList(() => [
    { no: 1, name: "face_annotations", kind: "message", T: FaceAnnotation, repeated: true },
    { no: 2, name: "landmark_annotations", kind: "message", T: EntityAnnotation, repeated: true },
    { no: 3, name: "logo_annotations", kind: "message", T: EntityAnnotation, repeated: true },
    { no: 4, name: "label_annotations", kind: "message", T: EntityAnnotation, repeated: true },
    { no: 5, name: "text_annotations", kind: "message", T: EntityAnnotation, repeated: true },
    { no: 12, name: "full_text_annotation", kind: "message", T: TextAnnotation },
    { no: 6, name: "safe_search_annotation", kind: "message", T: SafeSearchAnnotation },
    { no: 8, name: "image_properties_annotation", kind: "message", T: ImageProperties },
    { no: 11, name: "crop_hints_annotation", kind: "message", T: CropHintsAnnotation },
    { no: 13, name: "web_detection", kind: "message", T: WebDetection },
    { no: 9, name: "error", kind: "message", T: Status },
  ]);

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): AnnotateImageResponse {
    return new AnnotateImageResponse().fromBinary(bytes, options);
  }

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): AnnotateImageResponse {
    return new AnnotateImageResponse().fromJson(jsonValue, options);
  }

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): AnnotateImageResponse {
    return new AnnotateImageResponse().fromJsonString(jsonString, options);
  }

  static equals(a: AnnotateImageResponse | PlainMessage<AnnotateImageResponse> | undefined, b: AnnotateImageResponse | PlainMessage<AnnotateImageResponse> | undefined): boolean {
    return proto3.util.equals(AnnotateImageResponse, a, b);
  }
}

/**
 * Multiple image annotation requests are batched into a single service call.
 *
 * @generated from message google.cloud.vision.v1.BatchAnnotateImagesRequest
 */
export class BatchAnnotateImagesRequest extends Message<BatchAnnotateImagesRequest> {
  /**
   * Individual image annotation requests for this batch.
   *
   * @generated from field: repeated google.cloud.vision.v1.AnnotateImageRequest requests = 1;
   */
  requests: AnnotateImageRequest[] = [];

  constructor(data?: PartialMessage<BatchAnnotateImagesRequest>) {
    super();
    proto3.util.initPartial(data, this);
  }

  static readonly runtime: typeof proto3 = proto3;
  static readonly typeName = "google.cloud.vision.v1.BatchAnnotateImagesRequest";
  static readonly fields: FieldList = proto3.util.newFieldList(() => [
    { no: 1, name: "requests", kind: "message", T: AnnotateImageRequest, repeated: true },
  ]);

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): BatchAnnotateImagesRequest {
    return new BatchAnnotateImagesRequest().fromBinary(bytes, options);
  }

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): BatchAnnotateImagesRequest {
    return new BatchAnnotateImagesRequest().fromJson(jsonValue, options);
  }

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): BatchAnnotateImagesRequest {
    return new BatchAnnotateImagesRequest().fromJsonString(jsonString, options);
  }

  static equals(a: BatchAnnotateImagesRequest | PlainMessage<BatchAnnotateImagesRequest> | undefined, b: BatchAnnotateImagesRequest | PlainMessage<BatchAnnotateImagesRequest> | undefined): boolean {
    return proto3.util.equals(BatchAnnotateImagesRequest, a, b);
  }
}

/**
 * Response to a batch image annotation request.
 *
 * @generated from message google.cloud.vision.v1.BatchAnnotateImagesResponse
 */
export class BatchAnnotateImagesResponse extends Message<BatchAnnotateImagesResponse> {
  /**
   * Individual responses to image annotation requests within the batch.
   *
   * @generated from field: repeated google.cloud.vision.v1.AnnotateImageResponse responses = 1;
   */
  responses: AnnotateImageResponse[] = [];

  constructor(data?: PartialMessage<BatchAnnotateImagesResponse>) {
    super();
    proto3.util.initPartial(data, this);
  }

  static readonly runtime: typeof proto3 = proto3;
  static readonly typeName = "google.cloud.vision.v1.BatchAnnotateImagesResponse";
  static readonly fields: FieldList = proto3.util.newFieldList(() => [
    { no: 1, name: "responses", kind: "message", T: AnnotateImageResponse, repeated: true },
  ]);

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): BatchAnnotateImagesResponse {
    return new BatchAnnotateImagesResponse().fromBinary(bytes, options);
  }

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): BatchAnnotateImagesResponse {
    return new BatchAnnotateImagesResponse().fromJson(jsonValue, options);
  }

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): BatchAnnotateImagesResponse {
    return new BatchAnnotateImagesResponse().fromJsonString(jsonString, options);
  }

  static equals(a: BatchAnnotateImagesResponse | PlainMessage<BatchAnnotateImagesResponse> | undefined, b: BatchAnnotateImagesResponse | PlainMessage<BatchAnnotateImagesResponse> | undefined): boolean {
    return proto3.util.equals(BatchAnnotateImagesResponse, a, b);
  }
}

